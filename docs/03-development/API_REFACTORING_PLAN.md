# 3D-NAV APIé‡æ„æ–¹æ¡ˆ

**æ—¥æœŸ**: 2026-02-17
**ç›®æ ‡**: æå–ç»Ÿä¸€çš„APIæ¥å£å±‚ï¼Œæ–¹ä¾¿ä»£ç ç»´æŠ¤å’Œå¯¹å¤–é›†æˆ

---

## 1. æ‰§è¡Œæ‘˜è¦

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦APIé‡æ„ï¼Ÿ

**å½“å‰é—®é¢˜**:
- âŒ å„æ¨¡å—ç›´æ¥è€¦åˆï¼Œéš¾ä»¥ç‹¬ç«‹æµ‹è¯•
- âŒ æ²¡æœ‰ç»Ÿä¸€çš„æ¥å£è§„èŒƒ
- âŒ å¯¹å¤–é›†æˆå›°éš¾ï¼ˆéœ€è¦äº†è§£å†…éƒ¨å®ç°ï¼‰
- âŒ ä»£ç ä¿®æ”¹å½±å“èŒƒå›´å¤§
- âŒ éš¾ä»¥æ›¿æ¢åº•å±‚å®ç°

**é‡æ„ç›®æ ‡**:
- âœ… ç»Ÿä¸€çš„APIæ¥å£å±‚
- âœ… æ¨¡å—è§£è€¦ï¼Œç‹¬ç«‹å¯æµ‹
- âœ… æ¸…æ™°çš„å¯¹å¤–æ¥å£
- âœ… æ˜“äºç»´æŠ¤å’Œæ‰©å±•
- âœ… æ”¯æŒå¤šç§å®ç°æ–¹å¼

### 1.2 é‡æ„èŒƒå›´

```
å½“å‰æ¶æ„:
ROS2 Node â†’ ç›´æ¥è°ƒç”¨å†…éƒ¨ç±» â†’ ç´§è€¦åˆ

é‡æ„å:
ROS2 Node â†’ APIæ¥å£å±‚ â†’ å†…éƒ¨å®ç° â†’ æ¾è€¦åˆ
```

**æ¶‰åŠæ¨¡å—**:
1. Semantic Perception API
2. Semantic Planner API
3. Scene Graph API
4. LLM Client API
5. Action Executor API

---

## 2. å½“å‰æ¶æ„åˆ†æ

### 2.1 Semantic Perception å½“å‰ç»“æ„

```python
# å½“å‰: perception_node.py ç›´æ¥ä½¿ç”¨å†…éƒ¨ç±»
class SemanticPerceptionNode(Node):
    def __init__(self):
        # ç›´æ¥å®ä¾‹åŒ–æ£€æµ‹å™¨
        if detector_type == "yolo_world":
            from .yolo_world_detector import YOLOWorldDetector
            self.detector = YOLOWorldDetector(...)

        # ç›´æ¥å®ä¾‹åŒ–CLIPç¼–ç å™¨
        from .clip_encoder import CLIPEncoder
        self.clip_encoder = CLIPEncoder(...)

        # ç›´æ¥å®ä¾‹åŒ–è¿½è¸ªå™¨
        from .instance_tracker import InstanceTracker
        self.tracker = InstanceTracker(...)

    def process_frame(self, image, depth):
        # ç›´æ¥è°ƒç”¨å†…éƒ¨æ–¹æ³•
        detections = self.detector.detect(image)
        features = self.clip_encoder.encode(image, detections)
        tracked = self.tracker.update(detections)
        return tracked
```

**é—®é¢˜**:
- Nodeç›´æ¥ä¾èµ–å…·ä½“å®ç°ç±»
- éš¾ä»¥æ›¿æ¢æ£€æµ‹å™¨ï¼ˆéœ€è¦ä¿®æ”¹Nodeä»£ç ï¼‰
- éš¾ä»¥å•ç‹¬æµ‹è¯•å„ç»„ä»¶
- å¯¹å¤–ä½¿ç”¨éœ€è¦äº†è§£å†…éƒ¨ç»†èŠ‚

### 2.2 Semantic Planner å½“å‰ç»“æ„

```python
# å½“å‰: planner_node.py ç›´æ¥ä½¿ç”¨å†…éƒ¨ç±»
class SemanticPlannerNode(Node):
    def __init__(self):
        # ç›´æ¥å®ä¾‹åŒ–Goal Resolver
        from .goal_resolver import GoalResolver
        self.goal_resolver = GoalResolver(...)

        # ç›´æ¥å®ä¾‹åŒ–Task Decomposer
        from .task_decomposer import TaskDecomposer
        self.task_decomposer = TaskDecomposer(...)

        # ç›´æ¥å®ä¾‹åŒ–LLM Client
        from .llm_client import create_llm_client
        self.llm_client = create_llm_client(...)

    def handle_instruction(self, instruction):
        # ç›´æ¥è°ƒç”¨å†…éƒ¨æ–¹æ³•
        tasks = self.task_decomposer.decompose(instruction)
        goal = self.goal_resolver.resolve(tasks[0], scene_graph)
        return goal
```

**é—®é¢˜**:
- Nodeç›´æ¥ä¾èµ–å…·ä½“å®ç°
- éš¾ä»¥æ›¿æ¢Goal Resolverå®ç°
- éš¾ä»¥å•ç‹¬æµ‹è¯•
- å¯¹å¤–ä½¿ç”¨å¤æ‚

---

## 3. APIé‡æ„è®¾è®¡

### 3.1 è®¾è®¡åŸåˆ™

1. **æ¥å£ä¸å®ç°åˆ†ç¦»**: å®šä¹‰æŠ½è±¡æ¥å£ï¼Œéšè—å®ç°ç»†èŠ‚
2. **ä¾èµ–æ³¨å…¥**: é€šè¿‡æ„é€ å‡½æ•°æ³¨å…¥ä¾èµ–ï¼Œè€Œéç›´æ¥å®ä¾‹åŒ–
3. **å·¥å‚æ¨¡å¼**: ä½¿ç”¨å·¥å‚åˆ›å»ºå…·ä½“å®ç°
4. **ç»Ÿä¸€é”™è¯¯å¤„ç†**: å®šä¹‰ç»Ÿä¸€çš„å¼‚å¸¸ç±»å‹
5. **ç‰ˆæœ¬åŒ–**: APIæ”¯æŒç‰ˆæœ¬ç®¡ç†

### 3.2 ç›®å½•ç»“æ„

```
src/
â”œâ”€â”€ semantic_perception/
â”‚   â”œâ”€â”€ semantic_perception/
â”‚   â”‚   â”œâ”€â”€ api/                    # æ–°å¢ï¼šAPIæ¥å£å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ perception_api.py   # æ„ŸçŸ¥APIæ¥å£
â”‚   â”‚   â”‚   â”œâ”€â”€ detector_api.py     # æ£€æµ‹å™¨APIæ¥å£
â”‚   â”‚   â”‚   â”œâ”€â”€ encoder_api.py      # ç¼–ç å™¨APIæ¥å£
â”‚   â”‚   â”‚   â”œâ”€â”€ tracker_api.py      # è¿½è¸ªå™¨APIæ¥å£
â”‚   â”‚   â”‚   â””â”€â”€ factory.py          # å·¥å‚ç±»
â”‚   â”‚   â”œâ”€â”€ impl/                   # é‡å‘½åï¼šå®ç°å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ yolo_world_detector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ clip_encoder.py
â”‚   â”‚   â”‚   â”œâ”€â”€ instance_tracker.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ perception_node.py      # ä¿®æ”¹ï¼šä½¿ç”¨APIæ¥å£
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ semantic_planner/
â”‚   â”œâ”€â”€ semantic_planner/
â”‚   â”‚   â”œâ”€â”€ api/                    # æ–°å¢ï¼šAPIæ¥å£å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ planner_api.py      # è§„åˆ’APIæ¥å£
â”‚   â”‚   â”‚   â”œâ”€â”€ goal_resolver_api.py
â”‚   â”‚   â”‚   â”œâ”€â”€ task_decomposer_api.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_client_api.py
â”‚   â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â”‚   â”œâ”€â”€ impl/                   # é‡å‘½åï¼šå®ç°å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ goal_resolver.py
â”‚   â”‚   â”‚   â”œâ”€â”€ task_decomposer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ planner_node.py         # ä¿®æ”¹ï¼šä½¿ç”¨APIæ¥å£
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ semantic_common/                # æ–°å¢ï¼šå…¬å…±APIåŒ…
    â”œâ”€â”€ semantic_common/
    â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ scene_graph_api.py  # åœºæ™¯å›¾API
    â”‚   â”‚   â”œâ”€â”€ types.py            # å…¬å…±ç±»å‹å®šä¹‰
    â”‚   â”‚   â””â”€â”€ exceptions.py       # å…¬å…±å¼‚å¸¸å®šä¹‰
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ...
```

---

## 4. APIæ¥å£è®¾è®¡

### 4.1 Semantic Perception API

#### 4.1.1 PerceptionAPI (é¡¶å±‚æ¥å£)

```python
# src/semantic_perception/semantic_perception/api/perception_api.py

from abc import ABC, abstractmethod
from typing import List, Optional
from dataclasses import dataclass
import numpy as np

@dataclass
class Detection3D:
    """3Dæ£€æµ‹ç»“æœ"""
    id: str
    label: str
    confidence: float
    bbox_2d: List[float]  # [x1, y1, x2, y2]
    position_3d: List[float]  # [x, y, z]
    clip_feature: Optional[np.ndarray] = None


@dataclass
class SceneGraph:
    """åœºæ™¯å›¾"""
    objects: List[Detection3D]
    relations: List[dict]
    timestamp: float


class PerceptionAPI(ABC):
    """
    è¯­ä¹‰æ„ŸçŸ¥APIæ¥å£

    èŒè´£ï¼š
    - å¤„ç†RGB-Då›¾åƒ
    - è¾“å‡º3Dæ£€æµ‹ç»“æœå’Œåœºæ™¯å›¾
    """

    @abstractmethod
    def process_frame(
        self,
        rgb_image: np.ndarray,
        depth_image: np.ndarray,
        camera_info: dict,
        transform: Optional[np.ndarray] = None
    ) -> List[Detection3D]:
        """
        å¤„ç†å•å¸§å›¾åƒ

        Args:
            rgb_image: RGBå›¾åƒ (H, W, 3)
            depth_image: æ·±åº¦å›¾åƒ (H, W)
            camera_info: ç›¸æœºå†…å‚ {fx, fy, cx, cy}
            transform: ç›¸æœºåˆ°ä¸–ç•Œåæ ‡ç³»çš„å˜æ¢çŸ©é˜µ (4x4)

        Returns:
            3Dæ£€æµ‹ç»“æœåˆ—è¡¨
        """
        pass

    @abstractmethod
    def get_scene_graph(self) -> SceneGraph:
        """
        è·å–å½“å‰åœºæ™¯å›¾

        Returns:
            åœºæ™¯å›¾å¯¹è±¡
        """
        pass

    @abstractmethod
    def reset(self):
        """é‡ç½®æ„ŸçŸ¥ç³»ç»Ÿï¼ˆæ¸…ç©ºå†å²ï¼‰"""
        pass

    @abstractmethod
    def configure(self, config: dict):
        """
        é…ç½®æ„ŸçŸ¥ç³»ç»Ÿ

        Args:
            config: é…ç½®å­—å…¸
        """
        pass
```

#### 4.1.2 DetectorAPI (æ£€æµ‹å™¨æ¥å£)

```python
# src/semantic_perception/semantic_perception/api/detector_api.py

from abc import ABC, abstractmethod
from typing import List
from dataclasses import dataclass
import numpy as np

@dataclass
class Detection2D:
    """2Dæ£€æµ‹ç»“æœ"""
    label: str
    confidence: float
    bbox: List[float]  # [x1, y1, x2, y2]


class DetectorAPI(ABC):
    """
    ç‰©ä½“æ£€æµ‹å™¨APIæ¥å£

    æ”¯æŒçš„å®ç°ï¼š
    - YOLO-World
    - Grounding DINO
    - è‡ªå®šä¹‰æ£€æµ‹å™¨
    """

    @abstractmethod
    def detect(self, image: np.ndarray) -> List[Detection2D]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“

        Args:
            image: RGBå›¾åƒ (H, W, 3)

        Returns:
            2Dæ£€æµ‹ç»“æœåˆ—è¡¨
        """
        pass

    @abstractmethod
    def set_classes(self, classes: List[str]):
        """
        è®¾ç½®æ£€æµ‹ç±»åˆ«ï¼ˆå¼€æ”¾è¯æ±‡æ£€æµ‹ï¼‰

        Args:
            classes: ç±»åˆ«åç§°åˆ—è¡¨
        """
        pass

    @abstractmethod
    def get_model_info(self) -> dict:
        """
        è·å–æ¨¡å‹ä¿¡æ¯

        Returns:
            {name, version, input_size, ...}
        """
        pass
```

#### 4.1.3 EncoderAPI (ç¼–ç å™¨æ¥å£)

```python
# src/semantic_perception/semantic_perception/api/encoder_api.py

from abc import ABC, abstractmethod
from typing import List
import numpy as np

class EncoderAPI(ABC):
    """
    è§†è§‰-è¯­è¨€ç¼–ç å™¨APIæ¥å£

    æ”¯æŒçš„å®ç°ï¼š
    - CLIP
    - BLIP
    - è‡ªå®šä¹‰ç¼–ç å™¨
    """

    @abstractmethod
    def encode_image(self, image: np.ndarray) -> np.ndarray:
        """
        ç¼–ç å›¾åƒ

        Args:
            image: RGBå›¾åƒ (H, W, 3)

        Returns:
            å›¾åƒç‰¹å¾å‘é‡ (D,)
        """
        pass

    @abstractmethod
    def encode_text(self, text: str) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬

        Args:
            text: æ–‡æœ¬å­—ç¬¦ä¸²

        Returns:
            æ–‡æœ¬ç‰¹å¾å‘é‡ (D,)
        """
        pass

    @abstractmethod
    def compute_similarity(
        self,
        image_features: np.ndarray,
        text_features: np.ndarray
    ) -> float:
        """
        è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦

        Args:
            image_features: å›¾åƒç‰¹å¾ (D,)
            text_features: æ–‡æœ¬ç‰¹å¾ (D,)

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° [0, 1]
        """
        pass
```

#### 4.1.4 Factory (å·¥å‚ç±»)

```python
# src/semantic_perception/semantic_perception/api/factory.py

from typing import Optional
from .perception_api import PerceptionAPI
from .detector_api import DetectorAPI
from .encoder_api import EncoderAPI

class PerceptionFactory:
    """æ„ŸçŸ¥ç³»ç»Ÿå·¥å‚ç±»"""

    @staticmethod
    def create_perception(
        detector_type: str = "yolo_world",
        encoder_type: str = "clip",
        config: Optional[dict] = None
    ) -> PerceptionAPI:
        """
        åˆ›å»ºæ„ŸçŸ¥ç³»ç»Ÿ

        Args:
            detector_type: æ£€æµ‹å™¨ç±»å‹ (yolo_world | grounding_dino)
            encoder_type: ç¼–ç å™¨ç±»å‹ (clip | blip)
            config: é…ç½®å­—å…¸

        Returns:
            PerceptionAPIå®ä¾‹
        """
        from ..impl.perception_impl import PerceptionImpl

        detector = PerceptionFactory.create_detector(detector_type, config)
        encoder = PerceptionFactory.create_encoder(encoder_type, config)

        return PerceptionImpl(detector, encoder, config)

    @staticmethod
    def create_detector(
        detector_type: str,
        config: Optional[dict] = None
    ) -> DetectorAPI:
        """åˆ›å»ºæ£€æµ‹å™¨"""
        if detector_type == "yolo_world":
            from ..impl.yolo_world_detector import YOLOWorldDetector
            return YOLOWorldDetector(config)
        elif detector_type == "grounding_dino":
            from ..impl.grounding_dino_detector import GroundingDINODetector
            return GroundingDINODetector(config)
        else:
            raise ValueError(f"Unknown detector type: {detector_type}")

    @staticmethod
    def create_encoder(
        encoder_type: str,
        config: Optional[dict] = None
    ) -> EncoderAPI:
        """åˆ›å»ºç¼–ç å™¨"""
        if encoder_type == "clip":
            from ..impl.clip_encoder import CLIPEncoder
            return CLIPEncoder(config)
        else:
            raise ValueError(f"Unknown encoder type: {encoder_type}")
```

---

### 4.2 Semantic Planner API

#### 4.2.1 PlannerAPI (é¡¶å±‚æ¥å£)

```python
# src/semantic_planner/semantic_planner/api/planner_api.py

from abc import ABC, abstractmethod
from typing import Optional
from dataclasses import dataclass

@dataclass
class NavigationGoal:
    """å¯¼èˆªç›®æ ‡"""
    x: float
    y: float
    z: float
    label: str
    confidence: float
    reasoning: str


@dataclass
class PlannerStatus:
    """è§„åˆ’å™¨çŠ¶æ€"""
    state: str  # idle | planning | navigating | completed | failed
    current_task: Optional[str]
    progress: float  # [0, 1]
    message: str


class PlannerAPI(ABC):
    """
    è¯­ä¹‰è§„åˆ’APIæ¥å£

    èŒè´£ï¼š
    - æ¥æ”¶è‡ªç„¶è¯­è¨€æŒ‡ä»¤
    - è¾“å‡ºå¯¼èˆªç›®æ ‡
    - ç®¡ç†ä»»åŠ¡æ‰§è¡Œ
    """

    @abstractmethod
    def plan(
        self,
        instruction: str,
        scene_graph: dict,
        robot_position: Optional[dict] = None
    ) -> NavigationGoal:
        """
        è§„åˆ’å¯¼èˆªç›®æ ‡

        Args:
            instruction: è‡ªç„¶è¯­è¨€æŒ‡ä»¤
            scene_graph: åœºæ™¯å›¾JSON
            robot_position: æœºå™¨äººå½“å‰ä½ç½® {x, y, z}

        Returns:
            å¯¼èˆªç›®æ ‡
        """
        pass

    @abstractmethod
    def get_status(self) -> PlannerStatus:
        """è·å–è§„åˆ’å™¨çŠ¶æ€"""
        pass

    @abstractmethod
    def cancel(self):
        """å–æ¶ˆå½“å‰ä»»åŠ¡"""
        pass

    @abstractmethod
    def reset(self):
        """é‡ç½®è§„åˆ’å™¨"""
        pass
```

#### 4.2.2 GoalResolverAPI (ç›®æ ‡è§£ææ¥å£)

```python
# src/semantic_planner/semantic_planner/api/goal_resolver_api.py

from abc import ABC, abstractmethod
from typing import Optional
from dataclasses import dataclass

@dataclass
class GoalResult:
    """ç›®æ ‡è§£æç»“æœ"""
    action: str  # navigate | explore
    target_x: float
    target_y: float
    target_z: float
    target_label: str
    confidence: float
    reasoning: str
    path: str  # fast | slow


class GoalResolverAPI(ABC):
    """
    ç›®æ ‡è§£æAPIæ¥å£

    èŒè´£ï¼š
    - Fast-SlowåŒè¿›ç¨‹ç›®æ ‡è§£æ
    - åœºæ™¯å›¾åŒ¹é…
    - LLMæ¨ç†
    """

    @abstractmethod
    def resolve(
        self,
        instruction: str,
        scene_graph: str,
        robot_position: Optional[dict] = None
    ) -> GoalResult:
        """
        è§£æç›®æ ‡

        Args:
            instruction: è‡ªç„¶è¯­è¨€æŒ‡ä»¤
            scene_graph: åœºæ™¯å›¾JSONå­—ç¬¦ä¸²
            robot_position: æœºå™¨äººä½ç½®

        Returns:
            ç›®æ ‡è§£æç»“æœ
        """
        pass

    @abstractmethod
    def fast_resolve(
        self,
        instruction: str,
        scene_graph: str,
        robot_position: Optional[dict] = None
    ) -> Optional[GoalResult]:
        """
        Fast Pathè§£æï¼ˆæ— LLMï¼‰

        Returns:
            GoalResult or None (Noneè¡¨ç¤ºéœ€è¦Slow Path)
        """
        pass

    @abstractmethod
    def slow_resolve(
        self,
        instruction: str,
        scene_graph: str,
        robot_position: Optional[dict] = None
    ) -> GoalResult:
        """
        Slow Pathè§£æï¼ˆä½¿ç”¨LLMï¼‰

        Returns:
            GoalResult
        """
        pass
```

#### 4.2.3 LLMClientAPI (LLMå®¢æˆ·ç«¯æ¥å£)

```python
# src/semantic_planner/semantic_planner/api/llm_client_api.py

from abc import ABC, abstractmethod
from typing import List, Optional
from dataclasses import dataclass

@dataclass
class LLMMessage:
    """LLMæ¶ˆæ¯"""
    role: str  # system | user | assistant
    content: str


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    content: str
    model: str
    tokens_used: int
    latency_ms: float


class LLMClientAPI(ABC):
    """
    LLMå®¢æˆ·ç«¯APIæ¥å£

    æ”¯æŒçš„å®ç°ï¼š
    - OpenAI (GPT-4o, GPT-4o-mini)
    - Claude (Claude 3.5 Sonnet)
    - Qwen (é€šä¹‰åƒé—®)
    """

    @abstractmethod
    def chat(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.2,
        max_tokens: int = 1000
    ) -> LLMResponse:
        """
        èŠå¤©è¡¥å…¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            LLMå“åº”
        """
        pass

    @abstractmethod
    def get_model_info(self) -> dict:
        """
        è·å–æ¨¡å‹ä¿¡æ¯

        Returns:
            {provider, model, context_length, ...}
        """
        pass
```

---

## 5. ä½¿ç”¨ç¤ºä¾‹

### 5.1 Perception APIä½¿ç”¨

```python
# æ—§æ–¹å¼ï¼ˆç›´æ¥ä½¿ç”¨å†…éƒ¨ç±»ï¼‰
from semantic_perception.yolo_world_detector import YOLOWorldDetector
from semantic_perception.clip_encoder import CLIPEncoder

detector = YOLOWorldDetector(config)
encoder = CLIPEncoder()
detections = detector.detect(image)
features = encoder.encode_image(image)

# æ–°æ–¹å¼ï¼ˆä½¿ç”¨APIæ¥å£ï¼‰
from semantic_perception.api import PerceptionFactory

# åˆ›å»ºæ„ŸçŸ¥ç³»ç»Ÿ
perception = PerceptionFactory.create_perception(
    detector_type="yolo_world",
    encoder_type="clip",
    config=config
)

# å¤„ç†å›¾åƒ
detections = perception.process_frame(rgb, depth, camera_info, transform)

# è·å–åœºæ™¯å›¾
scene_graph = perception.get_scene_graph()
```

### 5.2 Planner APIä½¿ç”¨

```python
# æ—§æ–¹å¼ï¼ˆç›´æ¥ä½¿ç”¨å†…éƒ¨ç±»ï¼‰
from semantic_planner.goal_resolver import GoalResolver
from semantic_planner.llm_client import create_llm_client

llm_client = create_llm_client(config)
goal_resolver = GoalResolver(llm_client, config)
result = goal_resolver.resolve(instruction, scene_graph)

# æ–°æ–¹å¼ï¼ˆä½¿ç”¨APIæ¥å£ï¼‰
from semantic_planner.api import PlannerFactory

# åˆ›å»ºè§„åˆ’å™¨
planner = PlannerFactory.create_planner(
    goal_resolver_type="fast_slow",
    llm_provider="openai",
    config=config
)

# è§„åˆ’ç›®æ ‡
goal = planner.plan(instruction, scene_graph, robot_position)

# è·å–çŠ¶æ€
status = planner.get_status()
```

### 5.3 ROS2 Nodeä½¿ç”¨

```python
# æ–°çš„perception_node.py
from semantic_perception.api import PerceptionFactory, PerceptionAPI

class SemanticPerceptionNode(Node):
    def __init__(self):
        super().__init__("semantic_perception_node")

        # é€šè¿‡å·¥å‚åˆ›å»ºæ„ŸçŸ¥ç³»ç»Ÿ
        self.perception: PerceptionAPI = PerceptionFactory.create_perception(
            detector_type=self.get_parameter("detector_type").value,
            encoder_type=self.get_parameter("encoder_type").value,
            config=self._load_config()
        )

    def process_callback(self, rgb_msg, depth_msg):
        # ä½¿ç”¨APIæ¥å£
        detections = self.perception.process_frame(
            rgb_image, depth_image, camera_info, transform
        )

        # å‘å¸ƒç»“æœ
        self.publish_detections(detections)
```

---

## 6. å®æ–½è®¡åˆ’

### 6.1 é˜¶æ®µ1ï¼šAPIæ¥å£å®šä¹‰ï¼ˆ1å‘¨ï¼‰

**ä»»åŠ¡**:
1. åˆ›å»ºAPIç›®å½•ç»“æ„
2. å®šä¹‰æ‰€æœ‰APIæ¥å£
3. å®šä¹‰å…¬å…±ç±»å‹å’Œå¼‚å¸¸
4. ç¼–å†™APIæ–‡æ¡£

**äº§å‡º**:
- `src/semantic_perception/semantic_perception/api/`
- `src/semantic_planner/semantic_planner/api/`
- `src/semantic_common/`
- APIæ–‡æ¡£

### 6.2 é˜¶æ®µ2ï¼šå®ç°å±‚é‡æ„ï¼ˆ2å‘¨ï¼‰

**ä»»åŠ¡**:
1. å°†ç°æœ‰å®ç°ç§»åˆ°`impl/`ç›®å½•
2. å®ç°APIæ¥å£
3. åˆ›å»ºå·¥å‚ç±»
4. æ›´æ–°å•å…ƒæµ‹è¯•

**äº§å‡º**:
- `src/semantic_perception/semantic_perception/impl/`
- `src/semantic_planner/semantic_planner/impl/`
- å·¥å‚ç±»
- æ›´æ–°çš„æµ‹è¯•

### 6.3 é˜¶æ®µ3ï¼šNodeå±‚é‡æ„ï¼ˆ1å‘¨ï¼‰

**ä»»åŠ¡**:
1. æ›´æ–°perception_node.pyä½¿ç”¨API
2. æ›´æ–°planner_node.pyä½¿ç”¨API
3. æ›´æ–°launchæ–‡ä»¶
4. é›†æˆæµ‹è¯•

**äº§å‡º**:
- é‡æ„çš„Nodeä»£ç 
- æ›´æ–°çš„launchæ–‡ä»¶
- é›†æˆæµ‹è¯•é€šè¿‡

### 6.4 é˜¶æ®µ4ï¼šæ–‡æ¡£å’Œç¤ºä¾‹ï¼ˆ1å‘¨ï¼‰

**ä»»åŠ¡**:
1. ç¼–å†™APIä½¿ç”¨æ–‡æ¡£
2. åˆ›å»ºç¤ºä¾‹ä»£ç 
3. æ›´æ–°CLAUDE.md
4. æ›´æ–°README

**äº§å‡º**:
- APIä½¿ç”¨æŒ‡å—
- ç¤ºä¾‹ä»£ç 
- æ›´æ–°çš„æ–‡æ¡£

---

## 7. ä¼˜åŠ¿åˆ†æ

### 7.1 å¯¹å†…éƒ¨å¼€å‘çš„å¥½å¤„

1. **æ¨¡å—è§£è€¦**
   - å„æ¨¡å—ç‹¬ç«‹å¼€å‘
   - æ˜“äºå•å…ƒæµ‹è¯•
   - é™ä½ç»´æŠ¤æˆæœ¬

2. **æ˜“äºæ‰©å±•**
   - æ–°å¢æ£€æµ‹å™¨ï¼šå®ç°DetectorAPIå³å¯
   - æ–°å¢LLMï¼šå®ç°LLMClientAPIå³å¯
   - ä¸å½±å“ç°æœ‰ä»£ç 

3. **ä»£ç å¤ç”¨**
   - APIæ¥å£å¯åœ¨å¤šä¸ªé¡¹ç›®ä¸­å¤ç”¨
   - å®ç°å¯ä»¥ç‹¬ç«‹æ¼”è¿›

### 7.2 å¯¹å¤–éƒ¨é›†æˆçš„å¥½å¤„

1. **æ¸…æ™°çš„æ¥å£**
   - ä¸éœ€è¦äº†è§£å†…éƒ¨å®ç°
   - åªéœ€è¦çŸ¥é“APIæ¥å£
   - é™ä½å­¦ä¹ æˆæœ¬

2. **ç¨³å®šçš„API**
   - å†…éƒ¨å®ç°å¯ä»¥æ”¹å˜
   - APIæ¥å£ä¿æŒç¨³å®š
   - å‘åå…¼å®¹

3. **æ˜“äºé›†æˆ**
   ```python
   # å¤–éƒ¨é¡¹ç›®ä½¿ç”¨
   from semantic_perception.api import PerceptionFactory

   perception = PerceptionFactory.create_perception()
   detections = perception.process_frame(rgb, depth, camera_info)
   ```

### 7.3 å¯¹è®ºæ–‡å‘è¡¨çš„å¥½å¤„

1. **æ¸…æ™°çš„ç³»ç»Ÿæ¶æ„**
   - APIå±‚æ¬¡æ¸…æ™°
   - æ˜“äºåœ¨è®ºæ–‡ä¸­æè¿°
   - æå‡ç³»ç»Ÿå®Œæ•´æ€§

2. **æ˜“äºå¯¹æ¯”å®éªŒ**
   - å¯ä»¥è½»æ¾æ›¿æ¢ä¸åŒå®ç°
   - å¯¹æ¯”Fast Path vs Slow Path
   - å¯¹æ¯”ä¸åŒæ£€æµ‹å™¨

---

## 8. é£é™©å’ŒæŒ‘æˆ˜

### 8.1 é£é™©

1. **é‡æ„å·¥ä½œé‡**: éœ€è¦4-5å‘¨æ—¶é—´
2. **å…¼å®¹æ€§**: å¯èƒ½å½±å“ç°æœ‰ä»£ç 
3. **æ€§èƒ½å¼€é”€**: APIå±‚å¯èƒ½å¼•å…¥è½»å¾®æ€§èƒ½å¼€é”€
4. **å­¦ä¹ æ›²çº¿**: å›¢é˜Ÿéœ€è¦é€‚åº”æ–°æ¶æ„

### 8.2 ç¼“è§£æªæ–½

1. **æ¸è¿›å¼é‡æ„**: å…ˆé‡æ„ä¸€ä¸ªæ¨¡å—ï¼ŒéªŒè¯åå†æ¨å¹¿
2. **ä¿æŒå…¼å®¹**: ä¿ç•™æ—§æ¥å£ä¸€æ®µæ—¶é—´ï¼Œé€æ­¥è¿ç§»
3. **æ€§èƒ½æµ‹è¯•**: ç¡®ä¿APIå±‚å¼€é”€<1%
4. **æ–‡æ¡£å’ŒåŸ¹è®­**: æä¾›è¯¦ç»†æ–‡æ¡£å’Œç¤ºä¾‹

---

## 9. æ€»ç»“

### 9.1 æ ¸å¿ƒä»·å€¼

**APIé‡æ„å°†å¸¦æ¥**:
- âœ… ç»Ÿä¸€çš„æ¥å£è§„èŒƒ
- âœ… æ¨¡å—è§£è€¦å’Œç‹¬ç«‹æµ‹è¯•
- âœ… æ˜“äºç»´æŠ¤å’Œæ‰©å±•
- âœ… æ¸…æ™°çš„å¯¹å¤–æ¥å£
- âœ… æå‡ç³»ç»Ÿä¸“ä¸šæ€§

### 9.2 æ¨èæ–¹æ¡ˆ

**å»ºè®®é‡‡ç”¨æ¸è¿›å¼é‡æ„**:
1. å…ˆé‡æ„Semantic Perceptionï¼ˆ2å‘¨ï¼‰
2. éªŒè¯æ•ˆæœåé‡æ„Semantic Plannerï¼ˆ2å‘¨ï¼‰
3. æœ€åé‡æ„å…¶ä»–æ¨¡å—ï¼ˆ1å‘¨ï¼‰

**æ€»æ—¶é—´**: 5å‘¨

### 9.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³**: è¯„å®¡æœ¬æ–¹æ¡ˆï¼Œç¡®å®šæ˜¯å¦å®æ–½
2. **æœ¬å‘¨**: åˆ›å»ºAPIæ¥å£å®šä¹‰
3. **ä¸‹å‘¨**: å¼€å§‹å®ç°å±‚é‡æ„
4. **3å‘¨å**: å®Œæˆç¬¬ä¸€ä¸ªæ¨¡å—é‡æ„

---

**æ–‡æ¡£ä½ç½®**: `docs/03-development/API_REFACTORING_PLAN.md`
**çŠ¶æ€**: ğŸ“‹ æ–¹æ¡ˆè®¾è®¡å®Œæˆï¼Œç­‰å¾…è¯„å®¡
**é¢„è®¡å·¥ä½œé‡**: 5å‘¨
**ä¼˜å…ˆçº§**: ä¸­ï¼ˆä¸å½±å“è®ºæ–‡å‘è¡¨ï¼Œä½†æå‡ç³»ç»Ÿè´¨é‡ï¼‰
