"""
ç›®æ ‡è§£æå™¨ â€” å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ + åœºæ™¯å›¾ â†’ 3D ç›®æ ‡åæ ‡ã€‚

æ ¸å¿ƒæµç¨‹ (VLingNav 2026 åŒè¿›ç¨‹ + ESCA é€‰æ‹©æ€§ Grounding):

  â”Œâ”€ Fast Path (System 1, æ— éœ€ LLM):
  â”‚   åœºæ™¯å›¾ç›´æ¥åŒ¹é… â†’ é«˜ç½®ä¿¡åº¦ â†’ ç›´æ¥è¾“å‡ºåæ ‡
  â”‚   å‚è€ƒ: VLingNav (arXiv 2601.08665) AdaCoT æœºåˆ¶
  â”‚         OmniNav (ICLR 2026) Fast-Slow ç³»ç»Ÿ
  â”‚
  â””â”€ Slow Path (System 2, è°ƒç”¨ LLM):
      ESCA é€‰æ‹©æ€§ Grounding â†’ è¿‡æ»¤åœºæ™¯å›¾ â†’ LLM æ¨ç†
      å‚è€ƒ: ESCA/SGCLIP (NeurIPS 2025) é€‰æ‹©æ€§ grounding
            AdaNav (ICLR 2026) ä¸ç¡®å®šæ€§è‡ªé€‚åº”
      å¯é€‰: GPT-4o Vision è§†è§‰ç¡®è®¤ (VLMnav 2024)

ä¸ºä»€ä¹ˆ Fast Path é‡è¦:
  - VLingNav å‘ç° 70%+ çš„å¯¼èˆªæ­¥éª¤ç”¨ System 1 å³å¯å®Œæˆ
  - OmniNav çš„ Fast æ¨¡å—æ”¯æŒ 5 Hz æ§åˆ¶é¢‘ç‡
  - çœå» LLM API è°ƒç”¨ â†’ å»¶è¿Ÿä» ~2s é™åˆ° ~10ms, API è´¹ç”¨é™ä½ 90%
"""

import asyncio
import json
import logging
import math
import re
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple

import numpy as np

from .llm_client import LLMClientBase, LLMError, LLMConfig, create_llm_client
from .prompt_templates import (
    build_goal_resolution_prompt,
    build_exploration_prompt,
    build_vision_grounding_prompt,
)

try:
    from semantic_perception.topology_graph import TopologySemGraph
except ImportError:
    TopologySemGraph = None

logger = logging.getLogger(__name__)


# â”€â”€ å¤šæºç½®ä¿¡åº¦æƒé‡ (AdaNav ä¸ç¡®å®šæ€§èåˆ) â”€â”€
WEIGHT_LABEL_MATCH = 0.35       # æ ‡ç­¾æ–‡æœ¬åŒ¹é…
WEIGHT_CLIP_SIM = 0.35          # CLIP è§†è§‰-è¯­è¨€ç›¸ä¼¼åº¦
WEIGHT_DETECTOR_SCORE = 0.15    # æ£€æµ‹å™¨ç½®ä¿¡åº¦
WEIGHT_SPATIAL_HINT = 0.15      # ç©ºé—´å…³ç³»æç¤ºå‘½ä¸­


@dataclass
class GoalResult:
    """ç›®æ ‡è§£æç»“æœã€‚"""
    action: str                    # "navigate" | "explore"
    target_x: float = 0.0
    target_y: float = 0.0
    target_z: float = 0.0
    target_label: str = ""
    confidence: float = 0.0
    reasoning: str = ""
    is_valid: bool = False
    error: str = ""
    path: str = ""                 # "fast" | "slow" â€” æ ‡è®°èµ°äº†å“ªæ¡è·¯å¾„
    candidate_id: int = -1         # BA-HSG: å€™é€‰ç‰©ä½“ ID
    frame_id: str = "map"          # åæ ‡å¸§ (é»˜è®¤ map, ä¸ planner_node ä¸€è‡´)


@dataclass
class TargetHypothesis:
    """BA-HSG å¤šå‡è®¾ç›®æ ‡ä¿¡å¿µ (Â§3.4.3)ã€‚"""
    object_id: int
    label: str
    position: List[float]          # [x, y, z]
    score: float                   # Fast Path fused score
    credibility: float             # BA-HSG composite credibility
    room_match: float              # room-instruction compatibility
    posterior: float = 0.0         # P(this is the true target | history)
    verified: bool = False         # æ˜¯å¦å·²åˆ°è¾¾éªŒè¯è¿‡
    rejected: bool = False         # æ˜¯å¦å·²è¢«æ‹’ç»


class TargetBeliefManager:
    """BA-HSG å¤šå‡è®¾ç›®æ ‡ä¿¡å¿µç®¡ç†å™¨ (Â§3.4.3)ã€‚
    
    ç»´æŠ¤å€™é€‰ç›®æ ‡çš„åéªŒåˆ†å¸ƒï¼Œæ”¯æŒè´å¶æ–¯æ›´æ–°å’ŒæœŸæœ›ä»£ä»·é€‰æ‹©ã€‚
    è§£å†³å¤šå®ä¾‹æ­§ä¹‰é—®é¢˜ (e.g., å¤šæŠŠæ¤…å­, å¤šä¸ªé—¨)ã€‚
    """

    def __init__(self, gamma1: float = 1.0, gamma2: float = 0.5, gamma3: float = 0.3):
        self._hypotheses: List[TargetHypothesis] = []
        self._gamma1 = gamma1  # fused score weight
        self._gamma2 = gamma2  # credibility weight
        self._gamma3 = gamma3  # room match weight
        self._accept_threshold = 0.7  # åéªŒé˜ˆå€¼ â†’ ç¡®è®¤ç›®æ ‡

    def init_from_candidates(
        self,
        candidates: List[Dict],
        instruction: str = "",
    ) -> None:
        """ä» Fast Path å€™é€‰åˆ—è¡¨åˆå§‹åŒ–åéªŒã€‚"""
        self._hypotheses = []
        for c in candidates:
            belief = c.get("belief", {})
            h = TargetHypothesis(
                object_id=c.get("id", -1),
                label=c.get("label", ""),
                position=c.get("position", [0, 0, 0]),
                score=c.get("fused_score", 0.5),
                credibility=belief.get("credibility", 0.5) if isinstance(belief, dict) else 0.5,
                room_match=c.get("room_match", 0.5),
            )
            self._hypotheses.append(h)
        self._compute_posterior()

    def _compute_posterior(self) -> None:
        """è®¡ç®—å½’ä¸€åŒ–åéªŒ: p_i âˆ exp(Î³1Â·score + Î³2Â·cred + Î³3Â·room)ã€‚"""
        if not self._hypotheses:
            return
        import math
        log_scores = []
        for h in self._hypotheses:
            if h.rejected:
                log_scores.append(-100.0)
            else:
                log_scores.append(
                    self._gamma1 * h.score
                    + self._gamma2 * h.credibility
                    + self._gamma3 * h.room_match
                )
        max_ls = max(log_scores)
        exp_scores = [math.exp(ls - max_ls) for ls in log_scores]
        total = sum(exp_scores) or 1.0
        for h, es in zip(self._hypotheses, exp_scores):
            h.posterior = es / total

    def bayesian_update(self, object_id: int, detected: bool, clip_sim: float = 0.5) -> None:
        """è´å¶æ–¯éªŒè¯æ›´æ–°: åˆ°è¾¾å€™é€‰é™„è¿‘åè§‚æµ‹ç»“æœ (Â§3.4.3)ã€‚"""
        for h in self._hypotheses:
            if h.object_id == object_id:
                if detected and clip_sim > 0.7:
                    likelihood = 0.9
                elif detected:
                    likelihood = 0.4 + 0.3 * clip_sim
                else:
                    likelihood = 0.1
                    h.rejected = True
                h.posterior *= likelihood
                h.verified = True
            else:
                # å…¶ä»–å€™é€‰: å¦‚æœç›®æ ‡è¢«ç¡®è®¤åœ¨ object_id, å…¶ä»–åéªŒé™ä½
                if detected and clip_sim > 0.7:
                    h.posterior *= 0.3
        # é‡æ–°å½’ä¸€åŒ–
        total = sum(h.posterior for h in self._hypotheses) or 1.0
        for h in self._hypotheses:
            h.posterior /= total

    def select_next_target(
        self,
        robot_position: Optional[List[float]] = None,
        beta: float = 0.5,
        rho: float = 0.2,
    ) -> Optional[TargetHypothesis]:
        """æœŸæœ›ä»£ä»·é€‰æ‹©: argmin(nav_cost - Î²Â·posterior + ÏÂ·info_gain) (Â§3.4.3)ã€‚"""
        active = [h for h in self._hypotheses if not h.rejected and not h.verified]
        if not active:
            # æ‰€æœ‰å€™é€‰å·²éªŒè¯æˆ–æ‹’ç» â†’ è¿”å›åéªŒæœ€é«˜çš„å·²éªŒè¯å€™é€‰
            verified = [h for h in self._hypotheses if h.verified and not h.rejected]
            return max(verified, key=lambda h: h.posterior) if verified else None

        best = None
        best_utility = -float("inf")
        for h in active:
            nav_cost = 0.0
            if robot_position:
                dx = h.position[0] - robot_position[0]
                dy = h.position[1] - robot_position[1]
                nav_cost = (dx ** 2 + dy ** 2) ** 0.5
            # ä¿¡æ¯å¢ç›Š: åˆ°è¾¾è¯¥å€™é€‰åèƒ½åŒºåˆ†å…¶ä»–å€™é€‰çš„èƒ½åŠ›
            info_gain = 0.0
            for h2 in active:
                if h2.object_id != h.object_id:
                    d = sum((a - b) ** 2 for a, b in zip(h.position, h2.position)) ** 0.5
                    if d < 3.0:
                        info_gain += 0.3  # é™„è¿‘æœ‰å…¶ä»–å€™é€‰ â†’ ä¸€æ¬¡éªŒè¯å¯åŒºåˆ†å¤šä¸ª
            utility = beta * h.posterior - nav_cost / 10.0 + rho * info_gain
            if utility > best_utility:
                best_utility = utility
                best = h
        return best

    @property
    def best_hypothesis(self) -> Optional[TargetHypothesis]:
        """åéªŒæœ€é«˜çš„æœªæ‹’ç»å‡è®¾ã€‚"""
        active = [h for h in self._hypotheses if not h.rejected]
        return max(active, key=lambda h: h.posterior) if active else None

    @property
    def is_converged(self) -> bool:
        """åéªŒæ˜¯å¦å·²æ”¶æ•› (æœ€é«˜åéªŒ > é˜ˆå€¼)ã€‚"""
        best = self.best_hypothesis
        return best is not None and best.posterior > self._accept_threshold

    @property
    def num_active(self) -> int:
        return sum(1 for h in self._hypotheses if not h.rejected)


class GoalResolver:
    """
    ç›®æ ‡è§£æå™¨: VLingNav åŒè¿›ç¨‹ + ESCA é€‰æ‹©æ€§ Grounding + AdaNav ç½®ä¿¡åº¦èåˆã€‚
    """

    def __init__(
        self,
        primary_config: LLMConfig,
        fallback_config: Optional[LLMConfig] = None,
        confidence_threshold: float = 0.6,
        fast_path_threshold: float = 0.75,   # Fast è·¯å¾„æœ€ä½ç½®ä¿¡åº¦
        max_replan_attempts: int = 3,
    ):
        self._primary = create_llm_client(primary_config)
        self._fallback = (
            create_llm_client(fallback_config) if fallback_config else None
        )
        self._confidence_threshold = confidence_threshold
        self._fast_path_threshold = fast_path_threshold
        self._max_replan_attempts = max_replan_attempts

        # æ¢ç´¢çŠ¶æ€
        self._explored_directions: List[Dict[str, float]] = []
        self._explore_step_count = 0
        self._visited_room_ids: set = set()

        # BA-HSG: å¤šå‡è®¾ç›®æ ‡ä¿¡å¿µç®¡ç†å™¨
        self._belief_manager = TargetBeliefManager()

        # åˆ›æ–°4: è¯­ä¹‰å…ˆéªŒå¼•æ“ (æ‹“æ‰‘æ„ŸçŸ¥æ¢ç´¢)
        from .semantic_prior import SemanticPriorEngine
        self._semantic_prior_engine = SemanticPriorEngine()

        # åˆ›æ–°5: æ‹“æ‰‘è¯­ä¹‰å›¾ (Topology Semantic Graph)
        self._tsg: Optional["TopologySemGraph"] = None
        if TopologySemGraph is not None:
            self._tsg = TopologySemGraph()

    # ================================================================
    #  Fast Path â€” System 1 (VLingNav / OmniNav)
    # ================================================================

    def fast_resolve(
        self,
        instruction: str,
        scene_graph_json: str,
        robot_position: Optional[Dict[str, float]] = None,
        clip_encoder=None,
    ) -> Optional[GoalResult]:
        """
        Fast Path: åœºæ™¯å›¾ç›´æ¥åŒ¹é…, æ— éœ€ LLMã€‚

        å‚è€ƒ:
          - VLingNav (2026): AdaCoT â€” ç®€å•æƒ…å†µç”¨ System 1
          - OmniNav (ICLR 2026): Fast æ¨¡å— 5 Hz waypoint
          - AdaNav (ICLR 2026): é«˜ç¡®å®šæ€§ â†’ è·³è¿‡æ·±åº¦æ¨ç†

        åŒ¹é…ç®—æ³•:
          1. ä»åœºæ™¯å›¾æå–ç‰©ä½“åˆ—è¡¨
          2. æå–æŒ‡ä»¤ä¸»è¯­ (ç›®æ ‡ç‰©ä½“) ä¸ä¿®é¥°è¯­ (ç©ºé—´å‚è€ƒç‰©)
          3. å¤šæºè¯„åˆ†: ä¸»è¯­åŒ¹é… + CLIP + æ£€æµ‹åˆ† + ç©ºé—´å…³ç³»
          4. æœ€é«˜åˆ† > fast_path_threshold â†’ ç›´æ¥è¿”å›ç›®æ ‡
          5. å¦åˆ™ â†’ è¿”å› None, äº¤ç»™ Slow Path

        Args:
            instruction: ç”¨æˆ·æŒ‡ä»¤
            scene_graph_json: åœºæ™¯å›¾ JSON
            robot_position: å½“å‰ä½ç½®
            clip_encoder: CLIP ç¼–ç å™¨ (å¯é€‰)

        Returns:
            GoalResult or None (None = éœ€è¦ Slow Path)
        """
        try:
            sg = json.loads(scene_graph_json)
        except (json.JSONDecodeError, TypeError):
            return None

        objects = sg.get("objects", [])
        if not objects:
            return None

        # GAP: é€å±‚ CLIP ç­›é€‰ (FSR-VLN è·¯çº¿ A) â€” å…ˆç­› Room å†ç­› Object
        allowed_obj_ids = self._get_object_ids_in_top_rooms(
            instruction, sg, clip_encoder, top_k=2
        )
        if allowed_obj_ids is not None:  # None = æœªå¯ç”¨/æ—  rooms
            objects = [o for o in objects if o.get("id") in allowed_obj_ids]
            if not objects:
                return None

        relations = sg.get("relations", [])
        inst_lower = instruction.lower()

        # â”€â”€ æå–æŒ‡ä»¤å…³é”®è¯ â”€â”€
        keywords = self._extract_keywords(instruction)

        # â”€â”€ æå–ä¸»è¯­ (ç›®æ ‡) ä¸ä¿®é¥°è¯­ (ç©ºé—´å‚è€ƒç‰©) â”€â”€
        # "find chair near the door" â†’ subject="chair", modifier="door"
        # "å»é—¨æ—è¾¹çš„æ¤…å­"           â†’ subject="æ¤…å­",  modifier="é—¨"
        subject_labels, modifier_labels = self._parse_instruction_roles(
            inst_lower, keywords, [o.get("label", "").lower() for o in objects]
        )

        # â”€â”€ å¯¹æ¯ä¸ªç‰©ä½“æ‰“åˆ† (AdaNav å¤šæºç½®ä¿¡åº¦) â”€â”€
        scored: List[Tuple[dict, float, str]] = []

        for obj in objects:
            label = obj.get("label", "").lower()
            score = obj.get("score", 0.5)
            det_count = obj.get("detection_count", 1)

            # æº 1: æ ‡ç­¾æ–‡æœ¬åŒ¹é… â€” åŒºåˆ†ä¸»è¯­ vs ä¿®é¥°è¯­
            label_score = 0.0
            is_subject = False  # æ˜¯å¦åŒ¹é…ä¸ºæŒ‡ä»¤çš„ä¸»è¯­ (çœŸæ­£ç›®æ ‡)

            # ä¸»è¯­åŒ¹é… (ç›®æ ‡ç‰©ä½“, é«˜åˆ†)
            for subj in subject_labels:
                if subj == label:
                    label_score = 1.0
                    is_subject = True
                    break
                if subj in label or label in subj:
                    label_score = max(label_score, 0.9)
                    is_subject = True

            # å¦‚æœä¸æ˜¯ä¸»è¯­, æ£€æŸ¥æ˜¯å¦æ˜¯ä¿®é¥°è¯­ (ç©ºé—´å‚è€ƒç‰©, ä½åˆ†)
            if not is_subject:
                for mod in modifier_labels:
                    if mod == label or mod in label or label in mod:
                        label_score = max(label_score, 0.3)  # ä¿®é¥°è¯­ä½åˆ†
                        break

            # é€šç”¨å…³é”®è¯åŒ¹é… (å…œåº•)
            if label_score == 0.0:
                for kw in keywords:
                    if kw in label or label in kw:
                        label_score = max(label_score, 0.5)

            if label_score == 0.0:
                continue  # å®Œå…¨ä¸ç›¸å…³, è·³è¿‡

            # æº 2: æ£€æµ‹å™¨ç½®ä¿¡åº¦ (å¤šæ¬¡è§‚æµ‹ â†’ æ›´å¯é )
            detector_score = min(score, 1.0) * min(det_count / 3, 1.0)

            # æº 3: CLIP è§†è§‰-è¯­è¨€ç›¸ä¼¼åº¦
            clip_score = 0.0
            has_real_clip = False
            if clip_encoder is not None and obj.get("clip_feature") is not None:
                try:
                    clip_feature = np.array(obj.get("clip_feature"))
                    if clip_feature.size > 0:
                        similarities = clip_encoder.text_image_similarity(
                            instruction, [clip_feature]
                        )
                        clip_score = similarities[0] if similarities else 0.0
                        has_real_clip = True
                except Exception as e:
                    logger.warning("CLIP similarity computation failed: %s", e)
            # æ— çœŸå®CLIPæ—¶ä¸ä½¿ç”¨ä¼ªé€ è¿‘ä¼¼ â€” å°†æƒé‡é‡åˆ†é…ç»™å…¶ä»–çœŸå®ä¿¡å·

            # æº 4: ç©ºé—´å…³ç³»æç¤º
            # ä¿®å¤: åŒºåˆ†ä¸»ä½“(è¦æ‰¾çš„)å’Œå‚è€ƒç‰©(ç”¨äºå®šä½çš„)
            # "find chair near door" â†’ chairæ˜¯ä¸»ä½“, dooræ˜¯å‚è€ƒç‰©
            # åªæœ‰ä¸»ä½“åº”è¯¥è·å¾—ç©ºé—´å…³ç³»åŠ åˆ†
            spatial_score = 0.0
            for rel in relations:
                obj_id = obj.get("id")
                if rel.get("subject_id") == obj_id or rel.get("object_id") == obj_id:
                    # åˆ¤æ–­å½“å‰ç‰©ä½“æ˜¯å…³ç³»çš„ä¸»è¯­è¿˜æ˜¯å®¾è¯­
                    is_subject = (rel.get("subject_id") == obj_id)

                    related_id = (
                        rel["object_id"] if is_subject
                        else rel["subject_id"]
                    )
                    related_obj = next(
                        (o for o in objects if o.get("id") == related_id), None
                    )
                    if related_obj:
                        related_label = related_obj.get("label", "").lower()

                        # æå–æ ¸å¿ƒè¯ï¼ˆå»æ‰é¢œè‰²ç­‰ä¿®é¥°è¯ï¼‰
                        # "red chair" â†’ "chair", "blue door" â†’ "door"
                        label_core = self._extract_core_noun(label)
                        related_core = self._extract_core_noun(related_label)

                        # å…³é”®ä¿®å¤: æ£€æŸ¥æ ¸å¿ƒè¯æ˜¯å¦åœ¨æŒ‡ä»¤ä¸­
                        # "go to chair near door" åº”è¯¥åŒ¹é… "red chair" near "door"
                        label_in_inst = label_core in inst_lower or label in inst_lower
                        related_in_inst = related_core in inst_lower or related_label in inst_lower

                        if label_in_inst and related_in_inst:
                            # æ£€æŸ¥æŒ‡ä»¤ä¸­çš„è¯­ä¹‰: å“ªä¸ªæ˜¯ä¸»ä½“ï¼Œå“ªä¸ªæ˜¯å‚è€ƒ
                            # "find chair near door" â†’ "chair"åœ¨"door"å‰é¢ â†’ chairæ˜¯ä¸»ä½“
                            label_pos = inst_lower.find(label_core if label_core in inst_lower else label)
                            related_pos = inst_lower.find(related_core if related_core in inst_lower else related_label)

                            if label_pos < related_pos:
                                # å½“å‰ç‰©ä½“åœ¨å‰ â†’ æ˜¯ä¸»ä½“ â†’ ç»™é«˜åˆ†
                                spatial_score = 1.0
                                break
                            else:
                                # å½“å‰ç‰©ä½“åœ¨å â†’ æ˜¯å‚è€ƒç‰© â†’ ç»™ä½åˆ†
                                spatial_score = 0.2

                        # é€šç”¨è¿‘è·ç¦»å…³ç³»åŠ åˆ†(ä¿åº•)
                        elif rel.get("relation") == "near":
                            spatial_score = max(spatial_score, 0.3)

            # ç»¼åˆè¯„åˆ† (AdaNav é£æ ¼åŠ æƒèåˆ)
            if has_real_clip:
                # 4æºå®Œæ•´èåˆ
                fused_score = (
                    WEIGHT_LABEL_MATCH * label_score
                    + WEIGHT_CLIP_SIM * clip_score
                    + WEIGHT_DETECTOR_SCORE * detector_score
                    + WEIGHT_SPATIAL_HINT * spatial_score
                )
            else:
                # æ— CLIP: é‡åˆ†é…æƒé‡ â€” ä¸ä¼ªé€ æ•°æ®
                # æ ‡ç­¾åŒ¹é… 0.55, æ£€æµ‹å™¨ 0.25, ç©ºé—´ 0.20
                fused_score = (
                    0.55 * label_score
                    + 0.25 * detector_score
                    + 0.20 * spatial_score
                )

            clip_tag = f"clip={clip_score:.2f}" if has_real_clip else "clip=N/A"
            reason = (
                f"label={label_score:.1f}, {clip_tag}, "
                f"det={detector_score:.2f}, spatial={spatial_score:.1f} â†’ fused={fused_score:.2f}"
            )
            scored.append((obj, fused_score, reason))

        if not scored:
            return None

        # å–æœ€é«˜åˆ†
        scored.sort(key=lambda x: x[1], reverse=True)
        best_obj, best_score, best_reason = scored[0]

        # â”€â”€ B7: CLIP å±æ€§æ¶ˆæ­§ (åŒºåˆ† "red chair" vs "blue chair") â”€â”€
        # å½“å­˜åœ¨å¤šä¸ªåŒç±»å‹é«˜åˆ†å€™é€‰æ—¶, ç”¨ CLIP åšç²¾ç»†æ’åº
        if clip_encoder is not None and len(scored) >= 2:
            top_candidates = [
                (obj, sc, r) for obj, sc, r in scored[:5]
                if sc > best_score * 0.8
            ]
            if len(top_candidates) >= 2:
                # æ£€æŸ¥æ˜¯å¦åŒç±»å‹ (æ ¸å¿ƒåè¯ç›¸åŒ, åªæ˜¯å±æ€§ä¸åŒ)
                core_labels = [
                    self._extract_core_noun(o.get("label", "")).lower()
                    for o, _, _ in top_candidates
                ]
                if len(set(core_labels)) == 1:
                    # åŒç±»å‹å¤šä¸ªå€™é€‰: ç”¨ CLIP åŒºåˆ†å±æ€§
                    clip_ranked = self._clip_attribute_disambiguate(
                        instruction, top_candidates, clip_encoder
                    )
                    if clip_ranked:
                        best_obj, best_score, best_reason = clip_ranked[0]
                        best_reason += " [CLIP-attr-disambig]"

        # â”€â”€ è·ç¦»è¡°å‡ (è¿‘è·ç¦»ç›®æ ‡ä¼˜å…ˆ) â”€â”€
        if robot_position:
            pos = best_obj.get("position", {})
            dx = pos.get("x", 0) - robot_position.get("x", 0)
            dy = pos.get("y", 0) - robot_position.get("y", 0)
            dist = math.sqrt(dx * dx + dy * dy)
            # å¦‚æœæœ‰ç›¸è¿‘åˆ†æ•°ä½†æ›´è¿‘çš„å€™é€‰, è€ƒè™‘åˆ‡æ¢
            for obj2, sc2, _ in scored[1:3]:
                if sc2 > best_score * 0.9:  # åˆ†æ•°å·®è· < 10%
                    pos2 = obj2.get("position", {})
                    dx2 = pos2.get("x", 0) - robot_position.get("x", 0)
                    dy2 = pos2.get("y", 0) - robot_position.get("y", 0)
                    dist2 = math.sqrt(dx2 * dx2 + dy2 * dy2)
                    if dist2 < dist * 0.5:  # è¿‘ä¸€å€ä»¥ä¸Š â†’ åˆ‡æ¢
                        best_obj, best_score, best_reason = obj2, sc2, _
                        break

        # â”€â”€ åˆ¤æ–­æ˜¯å¦å¤Ÿæ ¼èµ° Fast Path â”€â”€
        if best_score < self._fast_path_threshold:
            logger.info(
                "Fast path score %.2f < threshold %.2f, deferring to Slow path. "
                "Best: %s (%s)",
                best_score, self._fast_path_threshold,
                best_obj.get("label", "?"), best_reason,
            )
            return None  # ä¸å¤Ÿç¡®å®š, äº¤ç»™ LLM

        # â”€â”€ BA-HSG: åˆå§‹åŒ–å¤šå‡è®¾ç›®æ ‡ä¿¡å¿µ â”€â”€
        if len(scored) >= 2:
            candidates_for_belief = []
            for obj_dict, fused_sc, _ in scored[:8]:
                pos_d = obj_dict.get("position", {})
                candidates_for_belief.append({
                    "id": obj_dict.get("id", -1),
                    "label": obj_dict.get("label", ""),
                    "position": [pos_d.get("x", 0), pos_d.get("y", 0), pos_d.get("z", 0)],
                    "fused_score": fused_sc,
                    "belief": obj_dict.get("belief", {}),
                    "room_match": 0.5,
                })
            self._belief_manager.init_from_candidates(candidates_for_belief, instruction)

            # ä½¿ç”¨ Belief-GoalNav å¤šå‡è®¾é€‰æ‹©
            robot_pos = [robot_position.get("x", 0), robot_position.get("y", 0)]
            selected = self._belief_manager.select_next_target(robot_pos)
            if selected and not self._belief_manager.is_converged:
                logger.info(
                    "ğŸ¯ BA-HSG multi-hypothesis: %d active candidates, "
                    "top posterior=%.3f (%s)",
                    self._belief_manager.num_active,
                    selected.posterior,
                    selected.label,
                )

        pos = best_obj.get("position", {})
        logger.info(
            "âš¡ Fast path hit: '%s' at (%.2f, %.2f), score=%.2f [%s]",
            best_obj.get("label", "?"),
            pos.get("x", 0), pos.get("y", 0),
            best_score, best_reason,
        )

        # ä»åœºæ™¯å›¾è·å–åæ ‡ç³»ï¼Œé»˜è®¤ä¸º map
        target_frame = sg.get("frame_id", "map")

        return GoalResult(
            action="navigate",
            target_x=pos.get("x", 0),
            target_y=pos.get("y", 0),
            target_z=pos.get("z", 0),
            target_label=best_obj.get("label", ""),
            confidence=best_score,
            reasoning=f"Fast path: {best_reason}",
            is_valid=True,
            path="fast",
            candidate_id=best_obj.get("id", -1),
            frame_id=target_frame,
        )

    @staticmethod
    def _extract_core_noun(label: str) -> str:
        """
        æå–æ ‡ç­¾çš„æ ¸å¿ƒåè¯ï¼ˆå»æ‰é¢œè‰²ç­‰ä¿®é¥°è¯ï¼‰ã€‚

        ä¾‹å¦‚:
            "red chair" â†’ "chair"
            "blue door" â†’ "door"
            "fire extinguisher" â†’ "fire extinguisher" (ä¿æŒä¸å˜)

        Args:
            label: ç‰©ä½“æ ‡ç­¾

        Returns:
            æ ¸å¿ƒåè¯
        """
        # å¸¸è§é¢œè‰²è¯
        colors = {
            "red", "blue", "green", "yellow", "white", "black", "gray", "grey",
            "orange", "purple", "pink", "brown", "cyan", "magenta",
            "çº¢è‰²", "è“è‰²", "ç»¿è‰²", "é»„è‰²", "ç™½è‰²", "é»‘è‰²", "ç°è‰²",
            "æ©™è‰²", "ç´«è‰²", "ç²‰è‰²", "æ£•è‰²", "çº¢", "è“", "ç»¿", "é»„", "ç™½", "é»‘", "ç°"
        }

        # åˆ†è¯
        words = label.split()

        # å»æ‰é¢œè‰²è¯
        core_words = [w for w in words if w.lower() not in colors]

        if core_words:
            return " ".join(core_words)
        else:
            # å¦‚æœå…¨æ˜¯é¢œè‰²è¯ï¼Œè¿”å›åŸæ ‡ç­¾
            return label

    @staticmethod
    def _get_object_ids_in_top_rooms(
        instruction: str,
        scene_graph: dict,
        clip_encoder,
        top_k: int = 2,
    ) -> Optional[set]:
        """
        GAP: Room CLIP ç­›é€‰ (FSR-VLN è·¯çº¿ A)ã€‚
        ç”¨ instructionâ†”Room ç›¸ä¼¼åº¦å– top-k æˆ¿é—´ï¼Œè¿”å›å…¶ object_ids é›†åˆã€‚
        æ—  clip/rooms æ—¶è¿”å› Noneï¼ˆä¸åšç­›é€‰ï¼‰ã€‚
        """
        if clip_encoder is None:
            return None
        subgraphs = scene_graph.get("subgraphs", [])
        rooms = [s for s in subgraphs if s.get("level") == "room"]
        if len(rooms) < 2:
            return None
        obj_by_id = {
            o.get("id"): o for o in scene_graph.get("objects", [])
            if o.get("id") is not None
        }
        room_texts = []
        for r in rooms:
            labels = r.get("object_labels", [])
            if not labels:
                oids = r.get("object_ids", [])
                labels = [obj_by_id.get(oid, {}).get("label", "") for oid in oids]
            room_texts.append(" ".join(str(l) for l in labels[:10] if l))
        if not room_texts:
            return None
        try:
            sims = clip_encoder.text_text_similarity(instruction, room_texts)
        except Exception:
            return None
        if not sims or len(sims) != len(rooms):
            return None
        ranked = sorted(range(len(rooms)), key=lambda i: sims[i], reverse=True)
        allowed = set()
        for i in ranked[:top_k]:
            allowed.update(rooms[i].get("object_ids", []))
        return allowed

    @staticmethod
    def _clip_attribute_disambiguate(
        instruction: str,
        candidates: List[Tuple[dict, float, str]],
        clip_encoder,
    ) -> List[Tuple[dict, float, str]]:
        """
        B7: ç”¨ CLIP å¯¹åŒç±»å‹å¤šå€™é€‰åšå±æ€§æ¶ˆæ­§ã€‚

        å½“åœºæ™¯ä¸­æœ‰ "red chair" å’Œ "blue chair" è€ŒæŒ‡ä»¤è¯´ "æ‰¾çº¢è‰²æ¤…å­" æ—¶,
        CLIP èƒ½é€šè¿‡è§†è§‰-è¯­è¨€å¯¹é½åŒºåˆ†å±æ€§ã€‚

        Args:
            instruction: ç”¨æˆ·æŒ‡ä»¤
            candidates: (object_dict, score, reason) åˆ—è¡¨
            clip_encoder: CLIPEncoder å®ä¾‹

        Returns:
            æŒ‰ CLIP ç›¸ä¼¼åº¦é‡æ’åçš„å€™é€‰åˆ—è¡¨
        """
        clip_scored = []
        for obj, fused_score, reason in candidates:
            clip_feature = obj.get("clip_feature")
            if clip_feature is not None:
                try:
                    feat = np.array(clip_feature)
                    if feat.size > 0:
                        sims = clip_encoder.text_image_similarity(
                            instruction, [feat]
                        )
                        clip_sim = sims[0] if sims else 0.0
                        # èåˆ: 70% åŸå§‹åˆ† + 30% CLIPå±æ€§åŒ¹é…
                        combined = 0.7 * fused_score + 0.3 * clip_sim
                        new_reason = (
                            f"{reason}, clip_attr={clip_sim:.3f}, "
                            f"combined={combined:.3f}"
                        )
                        clip_scored.append((obj, combined, new_reason))
                        continue
                except Exception:
                    pass
            clip_scored.append((obj, fused_score, reason))

        clip_scored.sort(key=lambda x: x[1], reverse=True)
        return clip_scored

    @staticmethod
    def _extract_keywords(instruction: str) -> List[str]:
        """
        ä»æŒ‡ä»¤ä¸­æå–å…³é”®è¯ (ä½¿ç”¨jiebaç²¾ç¡®åˆ†è¯)ã€‚

        å‡çº§è¯´æ˜ (P0ä»»åŠ¡):
        - åŸå®ç°: ç®€å•regexåˆ†è¯ï¼Œä¸­æ–‡æŒ‰å­—ç¬¦ç»„
        - æ–°å®ç°: jiebaç²¾ç¡®åˆ†è¯ï¼Œæ”¯æŒè‡ªå®šä¹‰è¯å…¸
        - å›é€€: jiebaæœªå®‰è£…æ—¶è‡ªåŠ¨å›é€€åˆ°ç®€å•åˆ†è¯

        å‚è€ƒ: SEMANTIC_NAV_REPORT.md ç¬¬11.1èŠ‚
        """
        import re

        stop_words = {
            "the", "a", "an", "to", "go", "find", "get", "me", "for", "and", "or",
            "is", "at", "in", "on", "near", "next", "by", "of", "with", "from",
            "å»", "åˆ°", "æ‰¾", "æ‹¿", "çš„", "åœ¨", "æ—è¾¹", "é™„è¿‘", "é‚£ä¸ª",
            "è¯·", "å¸®", "æˆ‘", "ä¸€ä¸ª", "æŠŠ", "äº†", "ç€", "è¿‡",
        }

        # å…ˆåˆ†ç¦»ä¸­è‹±æ–‡ â€” é¿å…jiebaåœ¨æ··åˆæ–‡æœ¬ä¸Šçš„åˆ‡åˆ†é”™è¯¯
        chinese_parts = re.findall(r'[\u4e00-\u9fff]+', instruction)
        english_parts = re.findall(r'[a-zA-Z]+', instruction.lower())
        chinese_text = " ".join(chinese_parts)

        all_keywords: List[str] = []

        # è‹±æ–‡: ç®€å•åˆ†è¯ (åœç”¨è¯è¿‡æ»¤ + é•¿åº¦è¿‡æ»¤)
        for w in english_parts:
            w_lower = w.lower()
            if w_lower not in stop_words and len(w_lower) > 1:
                all_keywords.append(w_lower)

        # ä¸­æ–‡: ä½¿ç”¨jiebaç²¾ç¡®åˆ†è¯ (å¦‚æœæœ‰çš„è¯)
        if chinese_text:
            try:
                from .chinese_tokenizer import extract_keywords
                zh_keywords = extract_keywords(
                    chinese_text,
                    min_length=2,
                    filter_stopwords=True,
                    keep_colors=True,
                    keep_spatial=True,
                )
                all_keywords.extend(zh_keywords)
            except ImportError:
                # å›é€€: ä¸­æ–‡æŒ‰è¿ç»­å­—ç¬¦ç»„
                zh_tokens = re.findall(r'[\u4e00-\u9fff]+', chinese_text)
                all_keywords.extend(
                    t for t in zh_tokens if t not in stop_words and len(t) > 1
                )

        return list(set(all_keywords))

    def _parse_instruction_roles(
        self,
        inst_lower: str,
        keywords: List[str],
        scene_labels: List[str],
    ) -> Tuple[List[str], List[str]]:
        """
        ä»æŒ‡ä»¤ä¸­è§£æä¸»è¯­ (å¯¼èˆªç›®æ ‡) å’Œä¿®é¥°è¯­ (ç©ºé—´å‚è€ƒç‰©)ã€‚

        B6 å‡çº§: ä¸‰çº§è§£æç­–ç•¥
          1. è§„åˆ™åŒ¹é… (regex) â€” æœ€å¿«, å¤„ç†å¸¸è§å¥å¼
          2. åœºæ™¯æ ‡ç­¾é¡ºåºåŒ¹é… â€” å…œåº•
          3. LLM å›é€€ â€” å¤æ‚æŒ‡ä»¤ (å¦‚ "æ‰¾åˆ°ä¹¦æˆ¿é‡Œé çª—çš„é‚£æŠŠçº¢è‰²æ¤…å­")

        Args:
            inst_lower: æŒ‡ä»¤å°å†™
            keywords: æå–çš„å…³é”®è¯
            scene_labels: åœºæ™¯å›¾ä¸­æ‰€æœ‰ç‰©ä½“æ ‡ç­¾ (å°å†™)

        Returns:
            (subject_labels, modifier_labels)
        """
        # ç¬¬ 1 çº§: è§„åˆ™åŒ¹é…
        subjects, modifiers = self._parse_roles_regex(inst_lower, keywords, scene_labels)
        if subjects:
            return subjects, modifiers

        # ç¬¬ 2 çº§: åœºæ™¯æ ‡ç­¾é¡ºåºåŒ¹é…
        subjects, modifiers = self._parse_roles_scene_order(inst_lower, keywords, scene_labels)
        if subjects:
            return subjects, modifiers

        # è·³è¿‡ Level 3 LLM: å½“å…³é”®è¯å’Œåœºæ™¯æ ‡ç­¾å®Œå…¨æ— äº¤é›†æ—¶ (å¦‚ä¸­è‹±è·¨è¯­è¨€)
        # LLM è§’è‰²è§£æä¸ä¼šæœ‰å¸®åŠ©, ä¸”æµªè´¹ API è°ƒç”¨
        has_any_overlap = any(
            kw in lbl or lbl in kw
            for kw in keywords
            for lbl in scene_labels
        )
        if not has_any_overlap:
            logger.debug(
                "Skipping LLM role parsing: no keyword-label overlap "
                "(cross-lingual scenario)"
            )
            return keywords[:], []

        # ç¬¬ 3 çº§: LLM å›é€€ (å¼‚æ­¥ â†’ åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è·‘)
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as pool:
                    future = pool.submit(
                        asyncio.run,
                        self._parse_roles_llm(inst_lower, scene_labels),
                    )
                    subjects, modifiers = future.result(timeout=5.0)
            else:
                subjects, modifiers = loop.run_until_complete(
                    self._parse_roles_llm(inst_lower, scene_labels)
                )
            if subjects:
                logger.info("LLM role parsing: subjects=%s, modifiers=%s", subjects, modifiers)
                return subjects, modifiers
        except Exception as e:
            logger.debug("LLM role parsing failed (non-critical): %s", e)

        # å…¨éƒ¨å½“ä¸»è¯­
        return keywords[:], []

    @staticmethod
    def _parse_roles_regex(
        inst_lower: str,
        keywords: List[str],
        scene_labels: List[str],
    ) -> Tuple[List[str], List[str]]:
        """ç¬¬ 1 çº§: æ­£åˆ™åŒ¹é…å¸¸è§å¥å¼ã€‚"""
        subjects: List[str] = []
        modifiers: List[str] = []

        en_patterns = [
            r'\b(?:find|go\s+to|navigate\s+to|locate|get)\s+([\w\s]+?)\s+(?:near|by|beside|next\s+to|behind|in\s+front\s+of|left\s+of|right\s+of|on|under|above|below)\s+(?:the\s+)?([\w\s]+)',
            r'\b([\w]+)\s+(?:near|by|beside|next\s+to)\s+(?:the\s+)?([\w]+)',
        ]

        for pat in en_patterns:
            m = re.search(pat, inst_lower)
            if m:
                subj_str = m.group(1).strip()
                mod_str = m.group(2).strip()
                for lbl in scene_labels:
                    if lbl in subj_str or subj_str in lbl:
                        subjects.append(lbl)
                    if lbl in mod_str or mod_str in lbl:
                        modifiers.append(lbl)
                if subjects:
                    return list(set(subjects)), list(set(modifiers))

        zh_patterns = [
            r'([\u4e00-\u9fff]+?)(?:æ—è¾¹|é™„è¿‘|å·¦è¾¹|å³è¾¹|å‰é¢|åé¢|ä¸Šé¢|ä¸‹é¢|å¯¹é¢|é‡Œé¢)çš„([\u4e00-\u9fff]+)',
            r'(?:å»|åˆ°|æ‰¾|æ‰¾åˆ°|å¯»æ‰¾)([\u4e00-\u9fff]+)',
        ]

        for i, pat in enumerate(zh_patterns):
            m = re.search(pat, inst_lower)
            if m:
                if i == 0:
                    mod_str = m.group(1)
                    subj_str = m.group(2)
                    for lbl in scene_labels:
                        if lbl in subj_str or subj_str in lbl:
                            subjects.append(lbl)
                        if lbl in mod_str or mod_str in lbl:
                            modifiers.append(lbl)
                    if subjects:
                        return list(set(subjects)), list(set(modifiers))
                else:
                    subj_str = m.group(1)
                    for lbl in scene_labels:
                        if lbl in subj_str or subj_str in lbl:
                            subjects.append(lbl)
                    if subjects:
                        return list(set(subjects)), []

        return [], []

    @staticmethod
    def _parse_roles_scene_order(
        inst_lower: str,
        keywords: List[str],
        scene_labels: List[str],
    ) -> Tuple[List[str], List[str]]:
        """ç¬¬ 2 çº§: æŒ‰æŒ‡ä»¤ä¸­å‡ºç°é¡ºåº, ç¬¬ä¸€ä¸ªåŒ¹é…åœºæ™¯ç‰©ä½“çš„è¯ä¸ºä¸»è¯­ã€‚"""
        found_in_scene = []
        for kw in keywords:
            for lbl in scene_labels:
                if kw in lbl or lbl in kw:
                    found_in_scene.append(lbl)
                    break
        for lbl in scene_labels:
            if lbl in inst_lower and lbl not in found_in_scene:
                found_in_scene.append(lbl)

        if found_in_scene:
            return [found_in_scene[0]], found_in_scene[1:]
        return [], []

    async def _parse_roles_llm(
        self,
        inst_lower: str,
        scene_labels: List[str],
    ) -> Tuple[List[str], List[str]]:
        """
        ç¬¬ 3 çº§: LLM è§£æå¤æ‚æŒ‡ä»¤ä¸­çš„ä¸»è¯­å’Œä¿®é¥°è¯­ã€‚

        B6 æ–°å¢: å¤„ç†è§„åˆ™æ— æ³•è¦†ç›–çš„å¤æ‚å¥å¼, ä¾‹å¦‚:
          - "æ‰¾åˆ°ä¹¦æˆ¿é‡Œé çª—çš„é‚£æŠŠçº¢è‰²æ¤…å­"
          - "go to the second room and find the cup on the table near the window"
        """
        labels_str = ", ".join(scene_labels[:30])
        messages = [
            {
                "role": "system",
                "content": (
                    "You are a robot navigation instruction parser. "
                    "Given an instruction and a list of visible object labels, "
                    "identify the TARGET objects (what the robot should go to) "
                    "and REFERENCE objects (spatial landmarks used for locating the target). "
                    "Reply in strict JSON: {\"targets\": [...], \"references\": [...]}"
                ),
            },
            {
                "role": "user",
                "content": (
                    f"Instruction: \"{inst_lower}\"\n"
                    f"Visible objects: [{labels_str}]\n"
                    f"Identify targets and references."
                ),
            },
        ]

        response = await self._call_with_fallback(messages)
        if not response:
            return [], []

        try:
            data = self._extract_json(response)
            targets = [
                str(t).lower() for t in data.get("targets", [])
                if isinstance(t, str)
            ]
            references = [
                str(r).lower() for r in data.get("references", [])
                if isinstance(r, str)
            ]
            # æ˜ å°„å›åœºæ™¯æ ‡ç­¾
            matched_targets = [
                lbl for lbl in scene_labels
                if any(t in lbl or lbl in t for t in targets)
            ]
            matched_refs = [
                lbl for lbl in scene_labels
                if any(r in lbl or lbl in r for r in references)
            ]
            return list(set(matched_targets)), list(set(matched_refs))
        except Exception:
            return [], []

    # ================================================================
    #  BA-HSG: å¤šå‡è®¾éªŒè¯ä¸é‡é€‰ (Â§3.4.3)
    # ================================================================

    def verify_and_reselect(
        self,
        object_id: int,
        detected: bool,
        clip_sim: float = 0.5,
        robot_position: Optional[List[float]] = None,
    ) -> Optional[GoalResult]:
        """åˆ°è¾¾å€™é€‰ç›®æ ‡åæ‰§è¡Œè´å¶æ–¯æ›´æ–°, å¦‚éœ€è¦åˆ™é‡é€‰ç›®æ ‡ã€‚
        
        Args:
            object_id: å½“å‰éªŒè¯çš„ç‰©ä½“ ID
            detected: åˆ°è¾¾åæ˜¯å¦æ£€æµ‹åˆ°ç›®æ ‡
            clip_sim: CLIP ç›¸ä¼¼åº¦
            robot_position: å½“å‰æœºå™¨äººä½ç½® [x, y]
            
        Returns:
            æ–°çš„ GoalResult å¦‚æœéœ€è¦é‡é€‰, å¦åˆ™ None (å½“å‰ç›®æ ‡ç¡®è®¤)
        """
        if not self._belief_manager._hypotheses:
            return None

        self._belief_manager.bayesian_update(object_id, detected, clip_sim)

        best = self._belief_manager.best_hypothesis
        if best is None:
            return None

        if self._belief_manager.is_converged and best.verified:
            logger.info(
                "âœ… BA-HSG belief converged: '%s' posterior=%.3f",
                best.label, best.posterior,
            )
            return None

        # éœ€è¦éªŒè¯ä¸‹ä¸€ä¸ªå€™é€‰
        next_target = self._belief_manager.select_next_target(robot_position)
        if next_target is None:
            return None

        logger.info(
            "ğŸ”„ BA-HSG reselect: switching to '%s' (posterior=%.3f, %d active)",
            next_target.label, next_target.posterior,
            self._belief_manager.num_active,
        )
        return GoalResult(
            action="navigate",
            target_x=next_target.position[0],
            target_y=next_target.position[1],
            target_z=next_target.position[2],
            target_label=next_target.label,
            confidence=next_target.posterior,
            reasoning=f"BA-HSG reselect: posterior={next_target.posterior:.3f}",
            is_valid=True,
            path="fast",
            candidate_id=next_target.object_id,
            frame_id="map",  # BA-HSG å¤šå‡è®¾éªŒè¯ä½¿ç”¨ map åæ ‡ç³»
        )

    # ================================================================
    #  Slow Path â€” System 2 (LLM æ¨ç†)
    # ================================================================

    async def resolve(
        self,
        instruction: str,
        scene_graph_json: str,
        robot_position: Optional[Dict[str, float]] = None,
        language: str = "zh",
        explore_if_unknown: bool = True,
        clip_encoder=None,
    ) -> GoalResult:
        """
        å®Œæ•´è§£æ (Fast â†’ Slow åŒè¿›ç¨‹)ã€‚

        æµç¨‹:
          1. å…ˆå°è¯• Fast Path (VLingNav System 1)
          2. Fast å¤±è´¥ â†’ é€‰æ‹©æ€§ Grounding è¿‡æ»¤åœºæ™¯å›¾ (ESCA)
          3. è¿‡æ»¤åçš„åœºæ™¯å›¾ â†’ LLM æ¨ç† (Slow Path)

        Args:
            instruction: ç”¨æˆ·è‡ªç„¶è¯­è¨€æŒ‡ä»¤
            scene_graph_json: åœºæ™¯å›¾ JSON
            robot_position: å½“å‰æœºå™¨äººä½ç½®
            language: "zh" / "en"
            explore_if_unknown: ç›®æ ‡æœªçŸ¥æ—¶æ˜¯å¦è‡ªåŠ¨æ¢ç´¢
            clip_encoder: CLIP ç¼–ç å™¨ (å¯é€‰)

        Returns:
            GoalResult
        """
        # â”€â”€ Step 1: Fast Path (VLingNav) â”€â”€
        fast_result = self.fast_resolve(
            instruction, scene_graph_json, robot_position, clip_encoder
        )
        if fast_result is not None:
            return fast_result

        # â”€â”€ Step 2: é€‰æ‹©æ€§ Grounding (ESCA + D1 CLIP æ’åº) â”€â”€
        filtered_sg = self._selective_grounding(
            instruction, scene_graph_json, clip_encoder=clip_encoder
        )

        # â”€â”€ Step 3: Slow Path (LLM) â”€â”€
        messages = build_goal_resolution_prompt(
            instruction, filtered_sg, robot_position, language
        )

        response_text = await self._call_with_fallback(messages)
        if response_text is None:
            return GoalResult(
                action="error",
                error="All LLM backends failed",
                is_valid=False,
            )

        result = self._parse_llm_response(response_text, json.loads(filtered_sg) if isinstance(filtered_sg, str) else filtered_sg)
        result.path = "slow"

        # å¦‚æœéœ€è¦æ¢ç´¢
        if (
            explore_if_unknown
            and result.action == "explore"
            and robot_position is not None
        ):
            logger.info(
                "Target unknown, generating exploration waypoint. "
                "Reason: %s", result.reasoning,
            )
            self._explored_directions.append(
                {"x": result.target_x, "y": result.target_y}
            )

        return result

    async def generate_exploration_waypoint(
        self,
        instruction: str,
        robot_position: Dict[str, float],
        step_distance: float = 2.0,
        language: str = "zh",
        scene_graph_json: str = "",
    ) -> GoalResult:
        """
        ç”Ÿæˆæ‹“æ‰‘æ„ŸçŸ¥æ¢ç´¢èˆªç‚¹ (åˆ›æ–°5 å‡çº§: TSG ä¿¡æ¯å¢ç›Šæ¢ç´¢)ã€‚

        åŒå±‚ç­–ç•¥:
          Layer 1 (TSG): å¦‚æœæ‹“æ‰‘è¯­ä¹‰å›¾å¯ç”¨, ç”¨ Algorithm 2 (Information Gain)
                         é€‰æ‹©æœ€ä¼˜æ¢ç´¢ç›®æ ‡, æ— éœ€ LLM è°ƒç”¨ (~1ms)
          Layer 2 (LLM): å¦‚æœ TSG ä¸å¯ç”¨/è¯„åˆ†è¿‡ä½, fallback åˆ° LLM æ¢ç´¢å»ºè®®

        å‚è€ƒ:
          - TopoNav (2025): æ‹“æ‰‘å›¾ä½œä¸ºç©ºé—´è®°å¿†
          - L3MVN (IROS 2023): LLM-guided frontier scoring
          - SG-Nav: å­å›¾è¯„åˆ†æ’å€¼åˆ° frontier
        """
        # â”€â”€ Layer 1: TSG ä¿¡æ¯å¢ç›Šæ¢ç´¢ (åˆ›æ–°5) â”€â”€
        tsg_result = self._try_tsg_exploration(
            instruction, robot_position, scene_graph_json, step_distance,
        )
        if tsg_result is not None:
            return tsg_result

        # â”€â”€ Layer 2: LLM æ¢ç´¢å»ºè®® (åˆ›æ–°4 åŸæ–¹æ¡ˆ) â”€â”€
        topology_context = None
        semantic_priors = None

        if scene_graph_json:
            try:
                sg = json.loads(scene_graph_json)
                rooms = sg.get("rooms", [])
                topology_edges = sg.get("topology_edges", [])

                if rooms:
                    priors = self._semantic_prior_engine.get_unexplored_priors(
                        target_instruction=instruction,
                        rooms=rooms,
                        topology_edges=topology_edges,
                        visited_room_ids=self._visited_room_ids,
                        current_room_id=self._get_current_room_id(
                            robot_position, rooms
                        ),
                    )
                    if priors:
                        semantic_priors = priors[:5]

                if topology_edges:
                    topo_parts = []
                    room_names = {
                        r.get("room_id", -1): r.get("name", "?")
                        for r in rooms
                    }
                    for te in topology_edges[:8]:
                        fr = te.get("from_room", -1)
                        to = te.get("to_room", -1)
                        fn = room_names.get(fr, f"room_{fr}")
                        tn = room_names.get(to, f"room_{to}")
                        visited_f = "âœ“" if fr in self._visited_room_ids else "?"
                        visited_t = "âœ“" if to in self._visited_room_ids else "?"
                        topo_parts.append(
                            f"{fn}[{visited_f}] â†” {tn}[{visited_t}] ({te.get('type', '?')})"
                        )
                    topology_context = "\n".join(topo_parts)

                # è¡¥å…… TSG æ‹“æ‰‘æ‘˜è¦åˆ° LLM ä¸Šä¸‹æ–‡
                if self._tsg:
                    tsg_context = self._tsg.to_prompt_context(language)
                    if tsg_context:
                        topology_context = (
                            (topology_context or "") + "\n" + tsg_context
                        ).strip()
            except (json.JSONDecodeError, TypeError, KeyError):
                pass

        messages = build_exploration_prompt(
            instruction, self._explored_directions, robot_position, language,
            topology_context=topology_context,
            semantic_priors=semantic_priors,
        )

        response_text = await self._call_with_fallback(messages)
        if response_text is None:
            angle = np.random.uniform(0, 2 * np.pi)
            return GoalResult(
                action="explore",
                target_x=robot_position["x"] + step_distance * np.cos(angle),
                target_y=robot_position["y"] + step_distance * np.sin(angle),
                target_z=robot_position.get("z", 0.0),
                reasoning="Random exploration (LLM unavailable)",
                confidence=0.1,
                is_valid=True,
            )

        try:
            data = self._extract_json(response_text)
            direction = data.get("explore_direction", {})
            dx = float(direction.get("x", 0)) - robot_position["x"]
            dy = float(direction.get("y", 0)) - robot_position["y"]
            norm = np.sqrt(dx**2 + dy**2)
            if norm > 0:
                dx = dx / norm * step_distance
                dy = dy / norm * step_distance

            self._explore_step_count += 1
            self._explored_directions.append({
                "x": robot_position["x"] + dx,
                "y": robot_position["y"] + dy,
            })

            return GoalResult(
                action="explore",
                target_x=robot_position["x"] + dx,
                target_y=robot_position["y"] + dy,
                target_z=robot_position.get("z", 0.0),
                reasoning=data.get("reasoning", "LLM exploration suggestion"),
                confidence=0.3,
                is_valid=True,
            )
        except Exception as e:
            logger.warning("Failed to parse exploration response: %s", e)
            return GoalResult(
                action="explore",
                error=str(e),
                is_valid=False,
            )

    def _try_tsg_exploration(
        self,
        instruction: str,
        robot_position: Dict[str, float],
        scene_graph_json: str,
        step_distance: float,
    ) -> Optional["GoalResult"]:
        """
        å°è¯•ä½¿ç”¨æ‹“æ‰‘è¯­ä¹‰å›¾ (TSG) é€‰æ‹©æ¢ç´¢ç›®æ ‡ã€‚

        å¦‚æœ TSG å¯ç”¨ä¸”è¿”å›é«˜ç½®ä¿¡åº¦æ¢ç´¢ç›®æ ‡, ç›´æ¥è¿”å› GoalResult;
        å¦åˆ™è¿”å› None è®©è°ƒç”¨è€… fallback åˆ° LLMã€‚
        """
        if self._tsg is None or not scene_graph_json:
            return None

        try:
            sg = json.loads(scene_graph_json)
        except (json.JSONDecodeError, TypeError):
            return None

        # åŒæ­¥ TSG
        self._tsg.update_from_scene_graph(sg)

        # æ³¨å…¥å‰æ²¿èŠ‚ç‚¹ (from scene graph)
        frontier_nodes = sg.get("frontier_nodes", [])
        if frontier_nodes:
            frontier_points = []
            frontier_sizes = []
            for fn in frontier_nodes:
                pos = fn.get("position", {})
                frontier_points.append(np.array([pos.get("x", 0), pos.get("y", 0)]))
                frontier_sizes.append(fn.get("frontier_size", 2.0))
            self._tsg.update_frontiers_from_costmap(frontier_points, frontier_sizes)

            # é¢„æµ‹å‰æ²¿æˆ¿é—´ç±»å‹ (åˆ©ç”¨è¯­ä¹‰å…ˆéªŒ)
            for fnode in self._tsg.frontiers:
                nearest_room_id = None
                for edge in self._tsg._adjacency.get(fnode.node_id, []):
                    other_id = (
                        edge.to_id if edge.from_id == fnode.node_id else edge.from_id
                    )
                    other = self._tsg.get_node(other_id)
                    if other and other.node_type == "room":
                        nearest_room_id = other.node_id
                        break
                if nearest_room_id is not None:
                    room_node = self._tsg.get_node(nearest_room_id)
                    if room_node:
                        fnode.predicted_room_type = self._predict_adjacent_room_type(
                            room_node.room_type
                        )

        # è®°å½•æœºå™¨äººä½ç½® (è§¦å‘æˆ¿é—´åˆ‡æ¢æ£€æµ‹)
        room_id = self._tsg.record_robot_position(
            robot_position["x"], robot_position["y"]
        )
        if room_id is not None:
            self._visited_room_ids.add(room_id)

        # è¿è¡Œ Algorithm 2: Information Gain Exploration
        targets = self._tsg.get_best_exploration_target(
            instruction, self._semantic_prior_engine, top_k=3,
        )

        if not targets:
            return None

        best = targets[0]

        # åªæœ‰è¯„åˆ†è¶³å¤Ÿé«˜æ‰ä½¿ç”¨ TSG ç»“æœ (é¿å…ä½è´¨é‡æ¢ç´¢)
        if best.score < 0.05:
            return None

        dx = best.position[0] - robot_position["x"]
        dy = best.position[1] - robot_position["y"]
        norm = np.sqrt(dx**2 + dy**2)
        if norm > step_distance:
            dx = dx / norm * step_distance
            dy = dy / norm * step_distance

        self._explore_step_count += 1
        self._explored_directions.append({
            "x": robot_position["x"] + dx,
            "y": robot_position["y"] + dy,
        })

        logger.info(
            "TSG exploration: %s (score=%.3f, IG=%.3f, hops=%d) â€” %s",
            best.node_name, best.score, best.information_gain,
            best.hops, best.reasoning,
        )

        # ä»åœºæ™¯å›¾è·å–åæ ‡ç³»
        frame_id = sg.get("frame_id", "map") if 'sg' in locals() else "map"

        return GoalResult(
            action="explore",
            target_x=robot_position["x"] + dx,
            target_y=robot_position["y"] + dy,
            target_z=robot_position.get("z", 0.0),
            target_label=best.node_name,
            reasoning=f"[TSG-IG] {best.reasoning}",
            confidence=min(0.5, best.score),
            is_valid=True,
            path="fast",
            frame_id=frame_id,
        )

    @staticmethod
    def _predict_adjacent_room_type(current_room_type: str) -> str:
        """åŸºäºå½“å‰æˆ¿é—´ç±»å‹é¢„æµ‹ç›¸é‚»æˆ¿é—´ç±»å‹ (ç©ºé—´å¸¸è¯†)ã€‚"""
        adjacency_priors = {
            "corridor": "office",
            "office": "corridor",
            "kitchen": "corridor",
            "meeting_room": "corridor",
            "bathroom": "corridor",
            "stairwell": "corridor",
            "lobby": "corridor",
            "storage": "corridor",
        }
        return adjacency_priors.get(current_room_type, "")

    async def vision_grounding(
        self,
        instruction: str,
        scene_graph_json: str,
        image_base64: str,
        language: str = "zh",
    ) -> Dict:
        """
        è§†è§‰ grounding â€” å‘é€ç›¸æœºå¸§ç»™ GPT-4o Visionã€‚

        å‚è€ƒ VLMnav (2024): å½“åœºæ™¯å›¾åŒ¹é…ç½®ä¿¡åº¦ä¸å¤Ÿæ—¶,
        ç›´æ¥è®© VLM çœ‹å›¾åˆ¤æ–­ç›®æ ‡æ˜¯å¦å¯è§ã€‚

        ä½¿ç”¨åœºæ™¯:
          - resolve() è¿”å›ä½ç½®ä¿¡åº¦æ—¶
          - æ¢ç´¢é˜¶æ®µåˆ°è¾¾æ–°è§†è§’æ—¶
          - ç”¨æˆ·æŒ‡ä»¤åŒ…å«è§†è§‰å±æ€§ ("çº¢è‰²çš„", "åæ‰çš„") æ—¶

        Args:
            instruction: ç”¨æˆ·æŒ‡ä»¤
            scene_graph_json: åœºæ™¯å›¾ JSON
            image_base64: JPEG base64 ç¼–ç 
            language: "zh" / "en"

        Returns:
            dict: {target_visible, position_in_frame, confidence, reasoning}
        """
        from .llm_client import OpenAIClient

        # Vision åªèƒ½ç”¨ OpenAI (GPT-4o) åç«¯
        client = None
        if isinstance(self._primary, OpenAIClient):
            client = self._primary
        elif self._fallback and isinstance(self._fallback, OpenAIClient):
            client = self._fallback

        if client is None or not hasattr(client, "chat_with_image"):
            logger.warning("No OpenAI client available for vision grounding")
            return {"target_visible": False, "confidence": 0.0, "reasoning": "No vision backend"}

        system, user_text = build_vision_grounding_prompt(
            instruction, scene_graph_json, language
        )

        try:
            response = await client.chat_with_image(
                text_prompt=user_text,
                image_base64=image_base64,
                system_prompt=system,
            )
            return self._extract_json(response)
        except Exception as e:
            logger.error("Vision grounding failed: %s", e)
            return {"target_visible": False, "confidence": 0.0, "reasoning": str(e)}

    # ================================================================
    #  é€‰æ‹©æ€§ Grounding (ESCA, NeurIPS 2025)
    # ================================================================

    def _selective_grounding(
        self,
        instruction: str,
        scene_graph_json: str,
        max_objects: int = 15,
        max_relations: int = 20,
        clip_encoder=None,
    ) -> str:
        """
        é€‰æ‹©æ€§ Grounding: åªç»™ LLM ä¸æŒ‡ä»¤ç›¸å…³çš„åœºæ™¯å­å›¾ã€‚

        D1 å‡çº§: CLIP è¯­ä¹‰æ’åºæ›¿æ¢çº¯å…³é”®è¯åŒ¹é…ã€‚

        å‚è€ƒ:
          - ESCA / SGCLIP (NeurIPS 2025):
            "selective grounding â€” identifying only contextually relevant
             objects and relationships"
          - MSGNav (2025): "key subgraph selection enables efficient reasoning"

        ç­–ç•¥:
          1. CLIP è¯­ä¹‰æ’åº (å¦‚å¯ç”¨): è®¡ç®—æ‰€æœ‰ç‰©ä½“æ ‡ç­¾ä¸æŒ‡ä»¤çš„è¯­ä¹‰ç›¸ä¼¼åº¦
          2. å…³é”®è¯åŒ¹é… (å…œåº•): æ ‡ç­¾å«æŒ‡ä»¤å…³é”®è¯çš„ç‰©ä½“ â†’ å¿…é€‰
          3. å…³ç³»é“¾æ‰©å±•: å¿…é€‰ç‰©ä½“çš„ 1-hop é‚»å±… â†’ åŠ å…¥ (SG-Nav)
          4. åŒºåŸŸå†…ç‰©ä½“: æŒ‡ä»¤æåˆ°çš„åŒºåŸŸ â†’ åŠ å…¥
          5. é™åˆ¶æ€»æ•°é¿å… token çˆ†ç‚¸

        Args:
            instruction: ç”¨æˆ·æŒ‡ä»¤
            scene_graph_json: å®Œæ•´åœºæ™¯å›¾ JSON
            max_objects: æœ€å¤šä¿ç•™ç‰©ä½“æ•°
            max_relations: æœ€å¤šä¿ç•™å…³ç³»æ•°
            clip_encoder: CLIP ç¼–ç å™¨ (å¯é€‰, D1 æ–°å¢)

        Returns:
            è¿‡æ»¤åçš„åœºæ™¯å›¾ JSON
        """
        try:
            sg = json.loads(scene_graph_json)
        except (json.JSONDecodeError, TypeError):
            return scene_graph_json

        objects = sg.get("objects", [])
        relations = sg.get("relations", [])
        regions = sg.get("regions", [])

        if len(objects) <= max_objects:
            return scene_graph_json

        keywords = self._extract_keywords(instruction)
        inst_lower = instruction.lower()

        # â”€â”€ D1: CLIP è¯­ä¹‰æ’åº (æ›¿ä»£çº¯å…³é”®è¯) â”€â”€
        clip_relevance: Dict[int, float] = {}
        if clip_encoder is not None:
            try:
                labels = [obj.get("label", "") for obj in objects]
                if labels:
                    # æ‰¹é‡è®¡ç®—æŒ‡ä»¤ä¸æ‰€æœ‰ç‰©ä½“æ ‡ç­¾çš„è¯­ä¹‰ç›¸ä¼¼åº¦
                    text_features = []
                    for lbl in labels:
                        text_features.append(lbl)
                    sims = clip_encoder.text_text_similarity(instruction, text_features)
                    if sims is None or len(sims) == 0:
                        # text_text ä¸å¯ç”¨, å°è¯• text_image
                        for idx, obj in enumerate(objects):
                            feat = obj.get("clip_feature")
                            if feat is not None:
                                f = np.array(feat)
                                if f.size > 0:
                                    s = clip_encoder.text_image_similarity(instruction, [f])
                                    if s:
                                        clip_relevance[obj.get("id", idx)] = s[0]
                    else:
                        for idx, (obj, sim) in enumerate(zip(objects, sims)):
                            clip_relevance[obj.get("id", idx)] = float(sim)
            except Exception as e:
                logger.debug("CLIP selective grounding failed (falling back to keywords): %s", e)

        # â”€â”€ ç¬¬ 1 è½®: CLIP + å…³é”®è¯è”åˆç­›é€‰ â”€â”€
        relevant_ids = set()
        relevance_scores: Dict[int, float] = {}

        for obj in objects:
            oid = obj.get("id")
            label = obj.get("label", "").lower()

            # CLIP ç›¸ä¼¼åº¦
            clip_sim = clip_relevance.get(oid, 0.0)

            # å…³é”®è¯åŒ¹é…åˆ†
            kw_score = 0.0
            if label in inst_lower or inst_lower in label:
                kw_score = 1.0
            else:
                for kw in keywords:
                    if kw in label or label in kw:
                        kw_score = max(kw_score, 0.8)

            # ç»¼åˆ: æœ‰ CLIP æ—¶ CLIP æƒé‡ 0.6 + å…³é”®è¯ 0.4; æ—  CLIP æ—¶çº¯å…³é”®è¯
            if clip_relevance:
                combined = 0.6 * clip_sim + 0.4 * kw_score
            else:
                combined = kw_score

            relevance_scores[oid] = combined
            if combined > 0.3:  # ç›¸å…³æ€§é˜ˆå€¼
                relevant_ids.add(oid)

        # â”€â”€ ç¬¬ 2 è½®: å…³ç³»é“¾ 1-hop æ‰©å±• (SG-Nav å±‚æ¬¡æ¨ç†) â”€â”€
        hop1_ids = set()
        for rel in relations:
            sid = rel.get("subject_id")
            oid = rel.get("object_id")
            if sid in relevant_ids:
                hop1_ids.add(oid)
            if oid in relevant_ids:
                hop1_ids.add(sid)
        relevant_ids |= hop1_ids

        # â”€â”€ ç¬¬ 3 è½®: åŒºåŸŸå†…ç‰©ä½“ (å¦‚æœæŒ‡ä»¤æåˆ°äº†åŒºåŸŸ) â”€â”€
        for region in regions:
            region_name = region.get("name", "").lower()
            if any(kw in region_name for kw in keywords):
                relevant_ids |= set(region.get("object_ids", []))

        # â”€â”€ ç¬¬ 4 è½®: å¦‚æœä»ç„¶ä¸ºç©º, å– CLIP æ’åº top + æœ€é«˜åˆ†ç‰©ä½“ â”€â”€
        if not relevant_ids:
            if relevance_scores:
                sorted_by_relevance = sorted(
                    objects,
                    key=lambda o: relevance_scores.get(o.get("id"), 0.0),
                    reverse=True,
                )
            else:
                sorted_by_relevance = sorted(
                    objects,
                    key=lambda o: (o.get("score", 0) * o.get("detection_count", 1)),
                    reverse=True,
                )
            relevant_ids = {o["id"] for o in sorted_by_relevance[:max_objects]}

        # â”€â”€ æ„å»ºè¿‡æ»¤åçš„åœºæ™¯å›¾ â”€â”€
        filtered_objects = [
            o for o in objects if o["id"] in relevant_ids
        ][:max_objects]
        filtered_obj_ids = {o["id"] for o in filtered_objects}

        filtered_relations = [
            r for r in relations
            if r.get("subject_id") in filtered_obj_ids
            and r.get("object_id") in filtered_obj_ids
        ][:max_relations]

        filtered_regions = [
            r for r in regions
            if any(oid in filtered_obj_ids for oid in r.get("object_ids", []))
        ]

        # é‡å»ºæ‘˜è¦
        summary = sg.get("summary", "")
        if filtered_objects:
            labels = [o["label"] for o in filtered_objects[:10]]
            summary = (
                f"Filtered {len(filtered_objects)}/{len(objects)} relevant objects: "
                + ", ".join(labels)
            )

        result = {
            "timestamp": sg.get("timestamp", 0),
            "object_count": len(filtered_objects),
            "objects": filtered_objects,
            "relations": filtered_relations,
            "regions": filtered_regions,
            "summary": summary,
            "_filter_note": f"ESCA selective grounding: {len(objects)}â†’{len(filtered_objects)} objects",
        }

        logger.debug(
            "Selective grounding: %dâ†’%d objects, %dâ†’%d relations",
            len(objects), len(filtered_objects),
            len(relations), len(filtered_relations),
        )

        return json.dumps(result, ensure_ascii=False)

    def reset_exploration(self):
        """é‡ç½®æ¢ç´¢çŠ¶æ€ (æ–°ä»»åŠ¡æ—¶è°ƒç”¨)ã€‚"""
        self._explored_directions.clear()
        self._explore_step_count = 0
        self._visited_room_ids.clear()
        if self._tsg is not None:
            self._tsg = TopologySemGraph() if TopologySemGraph is not None else None

    def update_visited_room(self, room_id: int) -> None:
        """æ ‡è®°æŸæˆ¿é—´å·²æ¢ç´¢ (æ‹“æ‰‘æ„ŸçŸ¥æ¢ç´¢ç”¨)ã€‚"""
        if room_id >= 0:
            self._visited_room_ids.add(room_id)

    @property
    def topology_graph(self) -> Optional["TopologySemGraph"]:
        """è·å–æ‹“æ‰‘è¯­ä¹‰å›¾å®ä¾‹ (ä¾›å¤–éƒ¨æ¨¡å—è®¿é—®)ã€‚"""
        return self._tsg

    @staticmethod
    def _get_current_room_id(
        robot_position: Dict[str, float],
        rooms: List[Dict],
    ) -> int:
        """æ ¹æ®æœºå™¨äººä½ç½®æ‰¾åˆ°å½“å‰æ‰€åœ¨çš„æˆ¿é—´ IDã€‚"""
        if not rooms:
            return -1
        robot_xy = np.array([
            robot_position.get("x", 0.0),
            robot_position.get("y", 0.0),
        ])
        best_id = -1
        best_dist = float("inf")
        for room in rooms:
            center = room.get("center", {})
            rx = float(center.get("x", 0.0))
            ry = float(center.get("y", 0.0))
            dist = float(np.linalg.norm(robot_xy - np.array([rx, ry])))
            if dist < best_dist:
                best_dist = dist
                best_id = room.get("room_id", -1)
        return best_id

    # ================================================================
    #  å†…éƒ¨æ–¹æ³•
    # ================================================================

    async def _call_with_fallback(
        self, messages: List[Dict[str, str]]
    ) -> Optional[str]:
        """è°ƒç”¨ä¸» LLM, å¤±è´¥åˆ™å°è¯•å¤‡ç”¨ã€‚"""
        # ä¸» LLM
        if self._primary.is_available():
            try:
                return await self._primary.chat(messages)
            except LLMError as e:
                logger.warning("Primary LLM failed: %s", e)

        # å¤‡ç”¨ LLM
        if self._fallback and self._fallback.is_available():
            try:
                logger.info("Trying fallback LLM...")
                return await self._fallback.chat(messages)
            except LLMError as e:
                logger.error("Fallback LLM also failed: %s", e)

        return None

    def _parse_llm_response(self, response_text: str, scene_graph: dict = None) -> GoalResult:
        """è§£æ LLM JSON å“åº”ã€‚"""
        try:
            data = self._extract_json(response_text)

            action = data.get("action", "navigate")
            target = data.get("target", {})

            # ä»åœºæ™¯å›¾è·å–åæ ‡ç³»ï¼Œé»˜è®¤ä¸º map
            frame_id = "map"
            if scene_graph is not None:
                frame_id = scene_graph.get("frame_id", "map")

            return GoalResult(
                action=action,
                target_x=float(target.get("x", 0)),
                target_y=float(target.get("y", 0)),
                target_z=float(target.get("z", 0)),
                target_label=data.get("target_label", ""),
                confidence=float(data.get("confidence", 0)),
                reasoning=data.get("reasoning", ""),
                is_valid=True,
                frame_id=frame_id,
            )
        except Exception as e:
            logger.error("Failed to parse LLM response: %s\nRaw: %s", e, response_text)
            return GoalResult(
                action="error",
                error=f"Parse error: {e}",
                is_valid=False,
            )

    @staticmethod
    def _extract_json(text: str) -> dict:
        """ä» LLM è¾“å‡ºä¸­æå– JSON (å¤„ç† markdown ä»£ç å— + æˆªæ–­ä¿®å¤)ã€‚"""
        candidates = []

        # å°è¯•æ‰¾ JSON ä»£ç å—
        match = re.search(r"```(?:json)?\s*([\s\S]*?)```", text)
        if match:
            candidates.append(match.group(1).strip())

        # æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1:
            candidates.append(text[start:end + 1])

        for candidate in candidates:
            try:
                return json.loads(candidate)
            except json.JSONDecodeError:
                pass

        # æˆªæ–­ä¿®å¤: LLM è¾“å‡ºå¯èƒ½åœ¨ reasoning å­—æ®µä¸­è¢«æˆªæ–­
        for candidate in candidates:
            try:
                fixed = GoalResolver._fix_truncated_json(candidate)
                if fixed:
                    return json.loads(fixed)
            except json.JSONDecodeError:
                pass

        raise ValueError(f"No JSON found in response: {text[:200]}")

    @staticmethod
    def _fix_truncated_json(text: str) -> Optional[str]:
        """å°è¯•ä¿®å¤è¢«æˆªæ–­çš„ JSON (å¸¸è§äº max_tokens é™åˆ¶)ã€‚"""
        required_keys = {"action", "target", "confidence"}
        try:
            json.loads(text)
            return text
        except json.JSONDecodeError:
            pass

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„é”®
        if not all(f'"{k}"' in text for k in required_keys):
            return None

        # å°è¯•åœ¨ä¸åŒä½ç½®æˆªæ–­å¹¶é—­åˆ JSON
        # ç­–ç•¥: æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„ key-value å¯¹, æˆªæ–­ reasoning å­—æ®µ
        reasoning_match = re.search(r'"reasoning"\s*:\s*"', text)
        if reasoning_match:
            prefix = text[:reasoning_match.start()]
            # æ£€æŸ¥ prefix ä¸­æ˜¯å¦æœ‰è¶³å¤Ÿçš„å­—æ®µ
            if '"confidence"' in prefix:
                cleaned = prefix.rstrip().rstrip(",")
                if not cleaned.endswith("}"):
                    cleaned += "}"
                try:
                    return json.loads(cleaned)
                except json.JSONDecodeError:
                    pass

        # ç­–ç•¥ 2: æš´åŠ›é—­åˆ
        for trim_pos in range(len(text) - 1, max(len(text) - 200, 0), -1):
            ch = text[trim_pos]
            if ch in (',', '"', '}'):
                snippet = text[:trim_pos]
                # é—­åˆæ‰“å¼€çš„å­—ç¬¦ä¸²
                if snippet.count('"') % 2 == 1:
                    snippet += '"'
                # é—­åˆæ‰“å¼€çš„å¯¹è±¡
                open_braces = snippet.count('{') - snippet.count('}')
                snippet += "}" * max(open_braces, 0)
                try:
                    result = json.loads(snippet)
                    if isinstance(result, dict) and required_keys.issubset(result.keys()):
                        return snippet
                except json.JSONDecodeError:
                    continue

        return None
