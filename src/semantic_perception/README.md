# Semantic Perception API

**ç‰ˆæœ¬**: 1.0.0
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
**å®Œæˆæ—¥æœŸ**: 2026-02-17

---

## ğŸ¯ æ¦‚è¿°

Semantic Perception APIæ˜¯3D-NAVé¡¹ç›®çš„æ ¸å¿ƒæ„ŸçŸ¥æ¨¡å—ï¼Œæä¾›äº†ä¸€ä¸ª**ä¸“ä¸šçº§çš„APIæ¥å£å±‚**ï¼Œç”¨äºå¤„ç†RGB-Då›¾åƒã€æ£€æµ‹ç‰©ä½“ã€æå–ç‰¹å¾ã€è¿½è¸ªå®ä¾‹å’Œæ„å»ºåœºæ™¯å›¾ã€‚

### æ ¸å¿ƒç‰¹æ€§

- âœ… **ç»Ÿä¸€çš„APIæ¥å£** - æ¸…æ™°çš„æŠ½è±¡å±‚
- âœ… **å·¥å‚æ¨¡å¼** - ç®€åŒ–åˆ›å»ºè¿‡ç¨‹
- âœ… **ä¾èµ–æ³¨å…¥** - æ¾è€¦åˆè®¾è®¡
- âœ… **å®Œæ•´ç±»å‹æ³¨è§£** - ç±»å‹å®‰å…¨
- âœ… **ç»Ÿä¸€å¼‚å¸¸å¤„ç†** - æ˜“äºè°ƒè¯•
- âœ… **æ¨¡å—åŒ–è®¾è®¡** - æ˜“äºæ‰©å±•

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install ultralytics open-clip-torch numpy
```

### åŸºæœ¬ä½¿ç”¨

```python
from semantic_perception.api import PerceptionFactory, CameraInfo
import numpy as np

# 1. åˆ›å»ºæ„ŸçŸ¥ç³»ç»Ÿ
perception = PerceptionFactory.create_perception(
    detector_type="yolo_world",
    encoder_type="clip"
)

# 2. è®¾ç½®æ£€æµ‹ç±»åˆ«
perception.detector.set_classes(["chair", "table", "person"])

# 3. å‡†å¤‡è¾“å…¥
rgb_image = np.zeros((480, 640, 3), dtype=np.uint8)
depth_image = np.zeros((480, 640), dtype=np.float32)
camera_info = CameraInfo(
    fx=525.0, fy=525.0,
    cx=319.5, cy=239.5,
    width=640, height=480
)

# 4. å¤„ç†å›¾åƒ
detections = perception.process_frame(rgb_image, depth_image, camera_info)

# 5. è·å–åœºæ™¯å›¾
scene_graph = perception.get_scene_graph()

print(f"æ£€æµ‹åˆ° {len(detections)} ä¸ªç‰©ä½“")
print(f"åœºæ™¯å›¾åŒ…å« {len(scene_graph.relations)} ä¸ªå…³ç³»")
```

---

## ğŸ“š APIæ–‡æ¡£

### æ ¸å¿ƒæ¥å£

#### PerceptionAPI

å®Œæ•´çš„æ„ŸçŸ¥ç³»ç»Ÿæ¥å£ã€‚

```python
from semantic_perception.api import PerceptionFactory

perception = PerceptionFactory.create_perception()

# å¤„ç†å›¾åƒ
detections = perception.process_frame(rgb, depth, camera_info)

# è·å–åœºæ™¯å›¾
scene_graph = perception.get_scene_graph()

# æŸ¥è¯¢ç‰©ä½“
chairs = perception.query_objects(label="chair", min_confidence=0.5)

# è·å–ç»Ÿè®¡
stats = perception.get_statistics()

# é‡ç½®ç³»ç»Ÿ
perception.reset()
```

#### DetectorAPI

ç‰©ä½“æ£€æµ‹æ¥å£ã€‚

```python
from semantic_perception.api import PerceptionFactory

detector = PerceptionFactory.create_detector("yolo_world")

# è®¾ç½®ç±»åˆ«
detector.set_classes(["chair", "table", "person"])

# æ£€æµ‹ç‰©ä½“
detections = detector.detect(image)

# è·å–æ¨¡å‹ä¿¡æ¯
info = detector.get_model_info()
```

#### EncoderAPI

ç‰¹å¾ç¼–ç æ¥å£ã€‚

```python
from semantic_perception.api import PerceptionFactory

encoder = PerceptionFactory.create_encoder("clip")

# ç¼–ç å›¾åƒ
image_feat = encoder.encode_image(image)

# ç¼–ç æ–‡æœ¬
text_feat = encoder.encode_text("a red chair")

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = encoder.compute_similarity(image_feat, text_feat)
```

#### TrackerAPI

å®ä¾‹è¿½è¸ªæ¥å£ã€‚

```python
from semantic_perception.api import PerceptionFactory

tracker = PerceptionFactory.create_tracker("instance")

# æ›´æ–°è¿½è¸ª
tracked = tracker.update(detections_3d)

# è·å–æ‰€æœ‰è¿½è¸ª
all_tracks = tracker.get_all_tracks()

# é‡ç½®è¿½è¸ªå™¨
tracker.reset()
```

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### å±‚æ¬¡ç»“æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PerceptionAPI               â”‚  â† é¡¶å±‚æ¥å£
â”‚  (process_frame, get_scene_graph)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ ä¾èµ–æ³¨å…¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DetectorAPI  EncoderAPI  TrackerAPIâ”‚  â† ç»„ä»¶æ¥å£
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ å®ç°
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YOLOWorld    CLIP    InstanceTrackerâ”‚  â† å…·ä½“å®ç°
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å¤„ç†æµç¨‹

```
RGB + Depth å›¾åƒ
    â†“
1. 2Dæ£€æµ‹ (DetectorAPI)
    â†“
2. CLIPç¼–ç  (EncoderAPI)
    â†“
3. 3DæŠ•å½± (å†…éƒ¨å®ç°)
    â†“
4. å®ä¾‹è¿½è¸ª (TrackerAPI)
    â†“
5. åœºæ™¯å›¾æ„å»º (å†…éƒ¨å®ç°)
    â†“
Detection3Dåˆ—è¡¨ + SceneGraph
```

---

## ğŸ“¦ ç›®å½•ç»“æ„

```
semantic_perception/
â”œâ”€â”€ api/                    # APIæ¥å£å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ types.py           # æ•°æ®ç±»å‹
â”‚   â”œâ”€â”€ exceptions.py      # å¼‚å¸¸å®šä¹‰
â”‚   â”œâ”€â”€ detector_api.py    # æ£€æµ‹å™¨æ¥å£
â”‚   â”œâ”€â”€ encoder_api.py     # ç¼–ç å™¨æ¥å£
â”‚   â”œâ”€â”€ tracker_api.py     # è¿½è¸ªå™¨æ¥å£
â”‚   â”œâ”€â”€ perception_api.py  # æ„ŸçŸ¥ç³»ç»Ÿæ¥å£
â”‚   â””â”€â”€ factory.py         # å·¥å‚ç±»
â”œâ”€â”€ impl/                   # å®ç°å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ yolo_world_detector.py
â”‚   â”œâ”€â”€ clip_encoder.py
â”‚   â”œâ”€â”€ instance_tracker.py
â”‚   â””â”€â”€ perception_impl.py
â””â”€â”€ examples/               # ç¤ºä¾‹ä»£ç 
    â””â”€â”€ api_usage_examples.py
```

---

## ğŸ“ è®¾è®¡æ¨¡å¼

### 1. å·¥å‚æ¨¡å¼

```python
# ä½¿ç”¨å·¥å‚åˆ›å»ºå¯¹è±¡
perception = PerceptionFactory.create_perception(
    detector_type="yolo_world",
    encoder_type="clip"
)
```

### 2. ä¾èµ–æ³¨å…¥

```python
# PerceptionImplé€šè¿‡æ„é€ å‡½æ•°æ³¨å…¥ä¾èµ–
class PerceptionImpl(PerceptionAPI):
    def __init__(
        self,
        detector: DetectorAPI,
        encoder: EncoderAPI,
        tracker: TrackerAPI
    ):
        self.detector = detector
        self.encoder = encoder
        self.tracker = tracker
```

### 3. ç­–ç•¥æ¨¡å¼

```python
# ä¸åŒçš„æ£€æµ‹å™¨å®ç°å¯ä»¥äº’æ¢
detector1 = PerceptionFactory.create_detector("yolo_world")
detector2 = PerceptionFactory.create_detector("grounding_dino")
```

---

## ğŸ”§ é…ç½®

### ä½¿ç”¨é…ç½®å¯¹è±¡

```python
from semantic_perception.api import PerceptionConfig, PerceptionFactory

config = PerceptionConfig(
    detector_type="yolo_world",
    encoder_type="clip",
    confidence_threshold=0.3,
    iou_threshold=0.5,
    merge_distance=0.5,
    max_depth=6.0,
    min_depth=0.3
)

perception = PerceptionFactory.create_from_config(config)
```

### è¿è¡Œæ—¶é‡é…ç½®

```python
# ä¿®æ”¹é…ç½®
new_config = PerceptionConfig(
    confidence_threshold=0.5,
    merge_distance=0.8
)

perception.configure(new_config)
```

---

## ğŸ› é”™è¯¯å¤„ç†

### å¼‚å¸¸å±‚æ¬¡

```
PerceptionAPIError
â”œâ”€â”€ DetectorError
â”‚   â”œâ”€â”€ DetectorInitError
â”‚   â””â”€â”€ DetectorInferenceError
â”œâ”€â”€ EncoderError
â”‚   â”œâ”€â”€ EncoderInitError
â”‚   â””â”€â”€ EncoderInferenceError
â”œâ”€â”€ TrackerError
â”œâ”€â”€ InvalidImageError
â”œâ”€â”€ InvalidDepthError
â””â”€â”€ ConfigurationError
```

### ä½¿ç”¨ç¤ºä¾‹

```python
from semantic_perception.api.exceptions import (
    InvalidImageError,
    PerceptionAPIError
)

try:
    detections = perception.process_frame(rgb, depth, camera_info)
except InvalidImageError as e:
    print(f"å›¾åƒæ ¼å¼é”™è¯¯: {e}")
except PerceptionAPIError as e:
    print(f"å¤„ç†å¤±è´¥: {e}")
```

---

## ğŸ“Š æ€§èƒ½

### åŸºå‡†æµ‹è¯•

| ç»„ä»¶ | å»¶è¿Ÿ | ååé‡ |
|------|------|--------|
| YOLO-Worldæ£€æµ‹ | ~50ms | 20 FPS |
| CLIPç¼–ç  | ~30ms | 33 FPS |
| å®ä¾‹è¿½è¸ª | ~5ms | 200 FPS |
| **æ€»è®¡** | ~85ms | **11 FPS** |

*æµ‹è¯•ç¯å¢ƒ: NVIDIA Jetson AGX Orin*

---

## ğŸ§ª æµ‹è¯•

### è¿è¡Œç¤ºä¾‹

```bash
cd src/semantic_perception
python examples/api_usage_examples.py
```

### è¿è¡Œå•å…ƒæµ‹è¯•

```bash
pytest src/semantic_perception/test -v
```

---

## ğŸ“– æ›´å¤šç¤ºä¾‹

æŸ¥çœ‹ `examples/api_usage_examples.py` è·å–7ä¸ªå®Œæ•´ç¤ºä¾‹ï¼š

1. åŸºæœ¬ä½¿ç”¨
2. å•ç‹¬ä½¿ç”¨ç»„ä»¶
3. æŸ¥è¯¢ç‰©ä½“
4. è·å–ç»Ÿè®¡ä¿¡æ¯
5. é…ç½®å’Œé‡é…ç½®
6. é”™è¯¯å¤„ç†
7. å·¥å‚æ–¹æ³•

---

## ğŸ¤ è´¡çŒ®

### æ·»åŠ æ–°çš„æ£€æµ‹å™¨

1. åˆ›å»ºæ–°ç±»ç»§æ‰¿ `DetectorAPI`
2. å®ç°æ‰€æœ‰æŠ½è±¡æ–¹æ³•
3. åœ¨ `factory.py` ä¸­æ³¨å†Œ

```python
class MyDetector(DetectorAPI):
    def detect(self, image: np.ndarray) -> List[Detection2D]:
        # å®ç°æ£€æµ‹é€»è¾‘
        pass
```

### æ·»åŠ æ–°çš„ç¼–ç å™¨

1. åˆ›å»ºæ–°ç±»ç»§æ‰¿ `EncoderAPI`
2. å®ç°æ‰€æœ‰æŠ½è±¡æ–¹æ³•
3. åœ¨ `factory.py` ä¸­æ³¨å†Œ

---

## ğŸ“ æ–‡æ¡£

- **APIæ–¹æ¡ˆ**: `docs/03-development/API_REFACTORING_PLAN.md`
- **å®ŒæˆæŠ¥å‘Š**: `docs/03-development/API_REFACTORING_COMPLETE.md`
- **æ€»ç»“æŠ¥å‘Š**: `docs/03-development/API_REFACTORING_SUMMARY.md`

---

## ğŸ“„ è®¸å¯è¯

MIT License

---

## ğŸ‘¥ ä½œè€…

3D-NAV Team

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š

- [YOLO-World](https://github.com/AILab-CVC/YOLO-World) - å¼€æ”¾è¯æ±‡æ£€æµ‹
- [OpenCLIP](https://github.com/mlfoundations/open_clip) - CLIPå®ç°
- [Ultralytics](https://github.com/ultralytics/ultralytics) - YOLOæ¡†æ¶

---

**ç‰ˆæœ¬**: 1.0.0
**æœ€åæ›´æ–°**: 2026-02-17
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
